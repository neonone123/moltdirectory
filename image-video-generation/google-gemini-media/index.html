<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Use the Gemini API (Nano Banana image generation, Veo video, Gemini TTS speech and audio understanding) to deliver">
  <title>google-gemini-media - MoltDirectory</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>ü¶û</text></svg>">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../../style.css">
  <script>
    // Apply saved theme before page renders to prevent flash
    (function() {
      const savedTheme = localStorage.getItem('theme') || 'dark';
      if (savedTheme === 'light') {
        document.documentElement.setAttribute('data-theme', 'light');
      }
    })();
  </script>
</head>
<body>
  <header class="header">
    <div class="header-inner">
      <a href="../../index.html" class="logo">

        <span class="logo-text">MoltDirectory</span>
      </a>
      <nav class="header-links">
        <a href="../../start-here/index.html" class="header-link">Start Here</a>
        <a href="/security-auditor" class="header-link">Security Auditor</a>
        <a href="https://github.com/neonone123/moltdirectory/issues/new?template=add-skill.yml" class="header-link" target="_blank" rel="noopener">Add Skill</a>
        <button class="theme-toggle" aria-label="Toggle theme" onclick="toggleTheme()">
          <svg class="icon-moon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/></svg>
          <svg class="icon-sun" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
        </button>
        <a href="https://www.reddit.com/r/OpenClawDirectory/" class="header-link" target="_blank" rel="noopener" aria-label="Community">
          <svg viewBox="0 0 16 16" fill="currentColor" width="28" height="28"><path d="M6.167 8a.83.83 0 0 0-.83.83c0 .459.372.84.83.831a.831.831 0 0 0 0-1.661m1.843 3.647c.315 0 1.403-.038 1.976-.611a.23.23 0 0 0 0-.306.213.213 0 0 0-.306 0c-.353.363-1.126.487-1.67.487-.545 0-1.308-.124-1.671-.487a.213.213 0 0 0-.306 0 .213.213 0 0 0 0 .306c.564.563 1.652.61 1.977.61zm.992-2.807c0 .458.373.83.831.83s.83-.381.83-.83a.831.831 0 0 0-1.66 0z"/><path d="M16 8A8 8 0 1 1 0 8a8 8 0 0 1 16 0m-3.828-1.165c-.315 0-.602.124-.812.325-.801-.573-1.9-.945-3.121-.993l.534-2.501 1.738.372a.83.83 0 1 0 .83-.869.83.83 0 0 0-.744.468l-1.938-.41a.2.2 0 0 0-.153.028.2.2 0 0 0-.086.134l-.592 2.788c-1.24.038-2.358.41-3.17.992-.21-.2-.496-.324-.81-.324a1.163 1.163 0 0 0-.478 2.224q-.03.17-.029.353c0 1.795 2.091 3.256 4.669 3.256s4.668-1.451 4.668-3.256c0-.114-.01-.238-.029-.353.401-.181.688-.592.688-1.069 0-.65-.525-1.165-1.165-1.165"/></svg>
        </a>
      </nav>
    </div>
  </header>
  
          <section class="skill-page-header">
            <div class="skill-page-header-inner">
              <a href="../index.html" class="back-link">‚Üê Back to Image & Video Generation</a>
              <div class="skill-page-meta">
                <a href="../index.html" class="skill-page-category">Image & Video Generation</a>
                <span class="skill-page-author">by @xsir0</span>
              </div>
              <h1 class="skill-page-title">google-gemini-media</h1>
              <p class="skill-page-desc">Use the Gemini API (Nano Banana image generation, Veo video, Gemini TTS speech and audio understanding) to deliver</p>
            </div>
          </section>
          <div class="skill-page-body">
            <article class="skill-page-content">
              <div class="skill-source-wrapper">
                <div class="skill-source-header">
                  <div class="skill-source-title">Source Code</div>
                  <button class="copy-btn" onclick="copySkillContent(this)">
                    <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="9" y="9" width="13" height="13" rx="2" ry="2"/><path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"/></svg>
                    Copy
                  </button>
                </div>
                <div class="markdown-content">
                  <h1>Gemini Multimodal Media (Image/Video/Speech) Skill</h1>
<h2>1. Goals and scope</h2>
<p>This Skill consolidates six Gemini API capabilities into reusable workflows and implementation templates:</p>
<ul>
<li>Image generation (Nano Banana: text-to-image, image editing, multi-turn iteration)</li>
<li>Image understanding (caption/VQA/classification/comparison, multi-image prompts; supports inline and Files API)</li>
<li>Video generation (Veo 3.1: text-to-video, aspect ratio/resolution control, reference-image guidance, first/last frames, video extension, native audio)</li>
<li>Video understanding (upload/inline/YouTube URL; summaries, Q&amp;A, timestamped evidence)</li>
<li>Speech generation (Gemini native TTS: single-speaker and multi-speaker; controllable style/accent/pace/tone)</li>
<li>Audio understanding (upload/inline; description, transcription, time-range transcription, token counting)</li>
</ul>
<blockquote>
<p>Convention: This Skill follows the official Google Gen AI SDK (Node.js/REST) as the main line; currently only Node.js/REST examples are provided. If your project already wraps other languages or frameworks, map this Skill&#39;s request structure, model selection, and I/O spec to your wrapper layer.</p>
</blockquote>
<hr>
<h2>2. Quick routing (decide which capability to use)</h2>
<ol>
<li><strong>Do you need to produce images?</strong></li>
</ol>
<ul>
<li>Need to generate images from scratch or edit based on an image -&gt; use <strong>Nano Banana image generation</strong> (see Section 5)</li>
</ul>
<ol start="2">
<li><strong>Do you need to understand images?</strong></li>
</ol>
<ul>
<li>Need recognition, description, Q&amp;A, comparison, or info extraction -&gt; use <strong>Image understanding</strong> (see Section 6)</li>
</ul>
<ol start="3">
<li><strong>Do you need to produce video?</strong></li>
</ol>
<ul>
<li>Need to generate an 8-second video (optionally with native audio) -&gt; use <strong>Veo 3.1 video generation</strong> (see Section 7)</li>
</ul>
<ol start="4">
<li><strong>Do you need to understand video?</strong></li>
</ol>
<ul>
<li>Need summaries/Q&amp;A/segment extraction with timestamps -&gt; use <strong>Video understanding</strong> (see Section 8)</li>
</ul>
<ol start="5">
<li><strong>Do you need to read text aloud?</strong></li>
</ol>
<ul>
<li>Need controllable narration, podcast/audiobook style, etc. -&gt; use <strong>Speech generation (TTS)</strong> (see Section 9)</li>
</ul>
<ol start="6">
<li><strong>Do you need to understand audio?</strong></li>
</ol>
<ul>
<li>Need audio descriptions, transcription, time-range transcription, token counting -&gt; use <strong>Audio understanding</strong> (see Section 10)</li>
</ul>
<hr>
<h2>3. Unified engineering constraints and I/O spec (must read)</h2>
<h3>3.0 Prerequisites (dependencies and tools)</h3>
<ul>
<li>Node.js 18+ (match your project version)</li>
<li>Install SDK (example):</li>
</ul>
<pre><code class="language-bash">npm install @google/genai
</code></pre>
<ul>
<li>REST examples only need <code>curl</code>; if you need to parse image Base64, install <code>jq</code> (optional).</li>
</ul>
<h3>3.1 Authentication and environment variables</h3>
<ul>
<li>Put your API key in <code>GEMINI_API_KEY</code></li>
<li>REST requests use <code>x-goog-api-key: $GEMINI_API_KEY</code></li>
</ul>
<h3>3.2 Two file input modes: Inline vs Files API</h3>
<p><strong>Inline (embedded bytes/Base64)</strong></p>
<ul>
<li>Pros: shorter call chain, good for small files.</li>
<li>Key constraint: total request size (text prompt + system instructions + embedded bytes) typically has a ~20MB ceiling.</li>
</ul>
<p><strong>Files API (upload then reference)</strong></p>
<ul>
<li>Pros: good for large files, reusing the same file, or multi-turn conversations.</li>
<li>Typical flow:<ol>
<li><code>files.upload(...)</code> (SDK) or <code>POST /upload/v1beta/files</code> (REST resumable)</li>
<li>Use <code>file_data</code> / <code>file_uri</code> in <code>generateContent</code></li>
</ol>
</li>
</ul>
<blockquote>
<p>Engineering suggestion: implement <code>ensure_file_uri()</code> so that when a file exceeds a threshold (for example 10-15MB warning) or is reused, you automatically route through the Files API.</p>
</blockquote>
<h3>3.3 Unified handling of binary media outputs</h3>
<ul>
<li><strong>Images</strong>: usually returned as <code>inline_data</code> (Base64) in response parts; in the SDK use <code>part.as_image()</code> or decode Base64 and save as PNG/JPG.</li>
<li><strong>Speech (TTS)</strong>: usually returns <strong>PCM</strong> bytes (Base64); save as <code>.pcm</code> or wrap into <code>.wav</code> (commonly 24kHz, 16-bit, mono).</li>
<li><strong>Video (Veo)</strong>: long-running async task; poll the operation; download the file (or use the returned URI).</li>
</ul>
<hr>
<h2>4. Model selection matrix (choose by scenario)</h2>
<blockquote>
<p>Important: model names, versions, limits, and quotas can change over time. Verify against official docs before use. Last updated: 2026-01-22.</p>
</blockquote>
<h3>4.1 Image generation (Nano Banana)</h3>
<ul>
<li><strong>gemini-2.5-flash-image</strong>: optimized for speed/throughput; good for frequent, low-latency generation/editing.</li>
<li><strong>gemini-3-pro-image-preview</strong>: stronger instruction following and high-fidelity text rendering; better for professional assets and complex edits.</li>
</ul>
<h3>4.2 General image/video/audio understanding</h3>
<ul>
<li>Docs use <code>gemini-3-flash-preview</code> for image, video, and audio understanding (choose stronger models as needed for quality/cost).</li>
</ul>
<h3>4.3 Video generation (Veo)</h3>
<ul>
<li>Example model: <code>veo-3.1-generate-preview</code> (generates 8-second video and can natively generate audio).</li>
</ul>
<h3>4.4 Speech generation (TTS)</h3>
<ul>
<li>Example model: <code>gemini-2.5-flash-preview-tts</code> (native TTS, currently in preview).</li>
</ul>
<hr>
<h2>5. Image generation (Nano Banana)</h2>
<h3>5.1 Text-to-Image</h3>
<p><strong>SDK (Node.js) minimal template</strong></p>
<pre><code class="language-js">import { GoogleGenAI } from &quot;@google/genai&quot;;
import * as fs from &quot;node:fs&quot;;

const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });

const response = await ai.models.generateContent({
  model: &quot;gemini-2.5-flash-image&quot;,
  contents:
    &quot;Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme&quot;,
});

const parts = response.candidates?.[0]?.content?.parts ?? [];
for (const part of parts) {
  if (part.text) console.log(part.text);
  if (part.inlineData?.data) {
    fs.writeFileSync(&quot;out.png&quot;, Buffer.from(part.inlineData.data, &quot;base64&quot;));
  }
}
</code></pre>
<p><strong>REST (with imageConfig) minimal template</strong></p>
<pre><code class="language-bash">curl -s -X POST   &quot;https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent&quot;   -H &quot;x-goog-api-key: $GEMINI_API_KEY&quot;   -H &quot;Content-Type: application/json&quot;   -d &#39;{
    &quot;contents&quot;:[{&quot;parts&quot;:[{&quot;text&quot;:&quot;Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme&quot;}]}],
    &quot;generationConfig&quot;: {&quot;imageConfig&quot;: {&quot;aspectRatio&quot;:&quot;16:9&quot;}}
  }&#39;
</code></pre>
<p><strong>REST image parsing (Base64 decode)</strong></p>
<pre><code class="language-bash">curl -s -X POST &quot;https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent&quot; \
  -H &quot;x-goog-api-key: $GEMINI_API_KEY&quot; \
  -H &quot;Content-Type: application/json&quot; \
  -d &#39;{&quot;contents&quot;:[{&quot;parts&quot;:[{&quot;text&quot;:&quot;A minimal studio product shot of a nano banana&quot;}]}]}&#39; \
  | jq -r &#39;.candidates[0].content.parts[] | select(.inline_data) | .inline_data.data&#39; \
  | base64 --decode &gt; out.png

# macOS can use: base64 -D &gt; out.png
</code></pre>
<h3>5.2 Text-and-Image-to-Image</h3>
<p>Use case: given an image, <strong>add/remove/modify elements</strong>, change style, color grading, etc.</p>
<p><strong>SDK (Node.js) minimal template</strong></p>
<pre><code class="language-js">import { GoogleGenAI } from &quot;@google/genai&quot;;
import * as fs from &quot;node:fs&quot;;

const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });

const prompt =
  &quot;Add a nano banana on the table, keep lighting consistent, cinematic tone.&quot;;
const imageBase64 = fs.readFileSync(&quot;input.png&quot;).toString(&quot;base64&quot;);

const response = await ai.models.generateContent({
  model: &quot;gemini-2.5-flash-image&quot;,
  contents: [
    { text: prompt },
    { inlineData: { mimeType: &quot;image/png&quot;, data: imageBase64 } },
  ],
});

const parts = response.candidates?.[0]?.content?.parts ?? [];
for (const part of parts) {
  if (part.inlineData?.data) {
    fs.writeFileSync(&quot;edited.png&quot;, Buffer.from(part.inlineData.data, &quot;base64&quot;));
  }
}
</code></pre>
<h3>5.3 Multi-turn image iteration (Multi-turn editing)</h3>
<p>Best practice: use chat for continuous iteration (for example: generate first, then &quot;only edit a specific region/element&quot;, then &quot;make variants in the same style&quot;).<br>To output mixed &quot;text + image&quot; results, set <code>response_modalities</code> to <code>[&quot;TEXT&quot;, &quot;IMAGE&quot;]</code>.</p>
<h3>5.4 ImageConfig</h3>
<p>You can set in <code>generationConfig.imageConfig</code> or the SDK config:</p>
<ul>
<li><code>aspectRatio</code>: e.g. <code>16:9</code>, <code>1:1</code>.</li>
<li><code>imageSize</code>: e.g. <code>2K</code>, <code>4K</code> (higher resolution is usually slower/more expensive and model support can vary).</li>
</ul>
<hr>
<h2>6. Image understanding (Image Understanding)</h2>
<h3>6.1 Two ways to provide input images</h3>
<ul>
<li><strong>Inline image data</strong>: suitable for small files (total request size &lt; 20MB).</li>
<li><strong>Files API upload</strong>: better for large files or reuse across multiple requests.</li>
</ul>
<h3>6.2 Inline images (Node.js) minimal template</h3>
<pre><code class="language-js">import { GoogleGenAI } from &quot;@google/genai&quot;;
import * as fs from &quot;node:fs&quot;;

const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });

const imageBase64 = fs.readFileSync(&quot;image.jpg&quot;).toString(&quot;base64&quot;);

const response = await ai.models.generateContent({
  model: &quot;gemini-3-flash-preview&quot;,
  contents: [
    { inlineData: { mimeType: &quot;image/jpeg&quot;, data: imageBase64 } },
    { text: &quot;Caption this image, and list any visible brands.&quot; },
  ],
});

console.log(response.text);
</code></pre>
<h3>6.3 Upload and reference with Files API (Node.js) minimal template</h3>
<pre><code class="language-js">import { GoogleGenAI, createPartFromUri, createUserContent } from &quot;@google/genai&quot;;

const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });
const uploaded = await ai.files.upload({ file: &quot;image.jpg&quot; });

const response = await ai.models.generateContent({
  model: &quot;gemini-3-flash-preview&quot;,
  contents: createUserContent([
    createPartFromUri(uploaded.uri, uploaded.mimeType),
    &quot;Caption this image.&quot;,
  ]),
});

console.log(response.text);
</code></pre>
<h3>6.4 Multi-image prompts</h3>
<p>Append multiple images as multiple <code>Part</code> entries in the same <code>contents</code>; you can mix uploaded references and inline bytes.</p>
<hr>
<h2>7. Video generation (Veo 3.1)</h2>
<h3>7.1 Core features (must know)</h3>
<ul>
<li>Generates <strong>8-second</strong> high-fidelity video, optionally 720p / 1080p / 4k, and supports native audio generation (dialogue, ambience, SFX).</li>
<li>Supports:<ul>
<li>Aspect ratio (16:9 / 9:16)</li>
<li>Video extension (extend a generated video; typically limited to 720p)</li>
<li>First/last frame control (frame-specific)</li>
<li>Up to 3 reference images (image-based direction)</li>
</ul>
</li>
</ul>
<h3>7.2 SDK (Node.js) minimal template: async polling + download</h3>
<pre><code class="language-js">import { GoogleGenAI } from &quot;@google/genai&quot;;

const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });

const prompt =
  &quot;A cinematic shot of a cat astronaut walking on the moon. Include subtle wind ambience.&quot;;
let operation = await ai.models.generateVideos({
  model: &quot;veo-3.1-generate-preview&quot;,
  prompt,
  config: { resolution: &quot;1080p&quot; },
});

while (!operation.done) {
  await new Promise((resolve) =&gt; setTimeout(resolve, 10_000));
  operation = await ai.operations.getVideosOperation({ operation });
}

const video = operation.response?.generatedVideos?.[0]?.video;
if (!video) throw new Error(&quot;No video returned&quot;);
await ai.files.download({ file: video, downloadPath: &quot;out.mp4&quot; });
</code></pre>
<h3>7.3 REST minimal template: predictLongRunning + poll + download</h3>
<p>Key point: Veo REST uses <code>:predictLongRunning</code> to return an operation name, then poll <code>GET /v1beta/{operation_name}</code>; once done, download from the video URI in the response.</p>
<h3>7.4 Common controls (recommend a unified wrapper)</h3>
<ul>
<li><code>aspectRatio</code>: <code>&quot;16:9&quot;</code> or <code>&quot;9:16&quot;</code></li>
<li><code>resolution</code>: <code>&quot;720p&quot; | &quot;1080p&quot; | &quot;4k&quot;</code> (higher resolutions are usually slower/more expensive)</li>
<li>When writing prompts: put dialogue in quotes; explicitly call out SFX and ambience; use cinematography language (camera position, movement, composition, lens effects, mood).</li>
<li>Negative constraints: if the API supports a negative prompt field, use it; otherwise list elements you do not want to see.</li>
</ul>
<h3>7.5 Important limits (engineering fallback needed)</h3>
<ul>
<li>Latency can vary from seconds to minutes; implement timeouts and retries.</li>
<li>Generated videos are only retained on the server for a limited time (download promptly).</li>
<li>Outputs include a SynthID watermark.</li>
</ul>
<p><strong>Polling fallback (with timeout/backoff) pseudocode</strong></p>
<pre><code class="language-js">const deadline = Date.now() + 300_000; // 5 min
let sleepMs = 2000;
while (!operation.done &amp;&amp; Date.now() &lt; deadline) {
  await new Promise((resolve) =&gt; setTimeout(resolve, sleepMs));
  sleepMs = Math.min(Math.floor(sleepMs * 1.5), 15_000);
  operation = await ai.operations.getVideosOperation({ operation });
}
if (!operation.done) throw new Error(&quot;video generation timed out&quot;);
</code></pre>
<hr>
<h2>8. Video understanding (Video Understanding)</h2>
<h3>8.1 Video input options</h3>
<ul>
<li><strong>Files API upload</strong>: recommended when file &gt; 100MB, video length &gt; ~1 minute, or you need reuse.</li>
<li><strong>Inline video data</strong>: for smaller files.</li>
<li><strong>Direct YouTube URL</strong>: can analyze public videos.</li>
</ul>
<h3>8.2 Files API (Node.js) minimal template</h3>
<pre><code class="language-js">import { GoogleGenAI, createPartFromUri, createUserContent } from &quot;@google/genai&quot;;

const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });
const uploaded = await ai.files.upload({ file: &quot;sample.mp4&quot; });

const response = await ai.models.generateContent({
  model: &quot;gemini-3-flash-preview&quot;,
  contents: createUserContent([
    createPartFromUri(uploaded.uri, uploaded.mimeType),
    &quot;Summarize this video. Provide timestamps for key events.&quot;,
  ]),
});

console.log(response.text);
</code></pre>
<h3>8.3 Timestamp prompting strategy</h3>
<ul>
<li>Ask for segmented bullets with &quot;(mm:ss)&quot; timestamps.</li>
<li>Require &quot;evidence with specific time ranges&quot; and include downstream structured extraction (JSON) in the same prompt if needed.</li>
</ul>
<hr>
<h2>9. Speech generation (Text-to-Speech, TTS)</h2>
<h3>9.1 Positioning</h3>
<ul>
<li>Native TTS: for &quot;precise reading + controllable style&quot; (podcasts, audiobooks, ad voiceover, etc.).</li>
<li>Distinguish from the Live API: Live API is more interactive and non-structured audio/multimodal conversation; TTS is focused on controlled narration.</li>
</ul>
<h3>9.2 Single-speaker TTS (Node.js) minimal template</h3>
<pre><code class="language-js">import { GoogleGenAI } from &quot;@google/genai&quot;;
import * as fs from &quot;node:fs&quot;;

const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });

const response = await ai.models.generateContent({
  model: &quot;gemini-2.5-flash-preview-tts&quot;,
  contents: [{ parts: [{ text: &quot;Say cheerfully: Have a wonderful day!&quot; }] }],
  config: {
    responseModalities: [&quot;AUDIO&quot;],
    speechConfig: {
      voiceConfig: {
        prebuiltVoiceConfig: { voiceName: &quot;Kore&quot; },
      },
    },
  },
});

const data =
  response.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data ?? &quot;&quot;;
if (!data) throw new Error(&quot;No audio returned&quot;);
fs.writeFileSync(&quot;out.pcm&quot;, Buffer.from(data, &quot;base64&quot;));
</code></pre>
<h3>9.3 Multi-speaker TTS (max 2 speakers)</h3>
<p>Requirements:</p>
<ul>
<li>Use <code>multiSpeakerVoiceConfig</code></li>
<li>Each speaker name must match the dialogue labels in the prompt (e.g., Joe/Jane).</li>
</ul>
<h3>9.4 Voice options and language</h3>
<ul>
<li><code>voice_name</code> supports 30 prebuilt voices (for example Zephyr, Puck, Charon, Kore, etc.).</li>
<li>The model can auto-detect input language and supports 24 languages (see docs for the list).</li>
</ul>
<h3>9.5 &quot;Director notes&quot; (strongly recommended for high-quality voice)</h3>
<p>Provide controllable directions for style, pace, accent, etc., but avoid over-constraining.</p>
<hr>
<h2>10. Audio understanding (Audio Understanding)</h2>
<h3>10.1 Typical tasks</h3>
<ul>
<li>Describe audio content (including non-speech like birds, alarms, etc.)</li>
<li>Generate transcripts</li>
<li>Transcribe specific time ranges</li>
<li>Count tokens (for cost estimates/segmentation)</li>
</ul>
<h3>10.2 Files API (Node.js) minimal template</h3>
<pre><code class="language-js">import { GoogleGenAI, createPartFromUri, createUserContent } from &quot;@google/genai&quot;;

const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });
const uploaded = await ai.files.upload({ file: &quot;sample.mp3&quot; });

const response = await ai.models.generateContent({
  model: &quot;gemini-3-flash-preview&quot;,
  contents: createUserContent([
    &quot;Describe this audio clip.&quot;,
    createPartFromUri(uploaded.uri, uploaded.mimeType),
  ]),
});

console.log(response.text);
</code></pre>
<h3>10.3 Key limits and engineering tips</h3>
<ul>
<li>Supports common formats: WAV/MP3/AIFF/AAC/OGG/FLAC.</li>
<li>Audio tokenization: about 32 tokens/second (about 1920 tokens per minute; values may change).</li>
<li>Total audio length per prompt is capped at 9.5 hours; multi-channel audio is downmixed; audio is resampled (see docs for exact parameters).</li>
<li>If total request size exceeds 20MB, you must use the Files API.</li>
</ul>
<hr>
<h2>11. End-to-end examples (composition)</h2>
<h3>Example A: Image generation -&gt; validation via understanding</h3>
<ol>
<li>Generate product images with Nano Banana (require negative space, consistent lighting).</li>
<li>Use image understanding for self-check: verify text clarity, brand spelling, and unsafe elements.</li>
<li>If not satisfied, feed the generated image into text+image editing and iterate.</li>
</ol>
<h3>Example B: Video generation -&gt; video understanding -&gt; narration script</h3>
<ol>
<li>Generate an 8-second shot with Veo (include dialogue or SFX).</li>
<li>Download and save (respect retention window).</li>
<li>Upload video to video understanding to produce a storyboard + timestamps + narration copy (then feed to TTS).</li>
</ol>
<h3>Example C: Audio understanding -&gt; time-range transcription -&gt; TTS redub</h3>
<ol>
<li>Upload meeting audio and transcribe full content.</li>
<li>Transcribe or summarize specific time ranges.</li>
<li>Use TTS to generate a &quot;broadcast&quot; version of the summary.</li>
</ol>
<hr>
<h2>12. Compliance and risk (must follow)</h2>
<ul>
<li>Ensure you have the necessary rights to upload images/video/audio; do not generate infringing, deceptive, harassing, or harmful content.</li>
<li>Generated images and videos include SynthID watermarking; videos may also have regional/person-based generation constraints.</li>
<li>Production systems must implement timeouts, retries, failure fallbacks, and human review/post-processing for generated content.</li>
</ul>
<hr>
<h2>13. Quick reference (Checklist)</h2>
<ul>
<li><input disabled="" type="checkbox"> Pick the right model: image generation (Flash Image / Pro Image Preview), video generation (Veo 3.1), TTS (Gemini 2.5 TTS), understanding (Gemini Flash/Pro).</li>
<li><input disabled="" type="checkbox"> Pick the right input mode: inline for small files; Files API for large/reuse.</li>
<li><input disabled="" type="checkbox"> Parse binary outputs correctly: image/audio via inline_data decode; video via operation polling + download.</li>
<li><input disabled="" type="checkbox"> For video generation: set aspectRatio / resolution, and download promptly (avoid expiration).</li>
<li><input disabled="" type="checkbox"> For TTS: set response_modalities=[&quot;AUDIO&quot;]; max 2 speakers; speaker names must match prompt.</li>
<li><input disabled="" type="checkbox"> For audio understanding: countTokens when needed; segment long audio or use Files API.</li>
</ul>

                </div>
              </div>
            </article>
            <aside class="skill-page-sidebar">
              <div class="sidebar-card">
                <div class="sidebar-title">Actions</div>
                <a href="https://github.com/openclaw/skills/tree/main/skills/xsir0/google-gemini-media/SKILL.md" target="_blank" rel="noopener" class="sidebar-btn sidebar-btn-primary">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/></svg>
                  View on GitHub
                </a>
                <a href="https://clawdhub.com/xsir0/google-gemini-media" target="_blank" rel="noopener" class="sidebar-btn sidebar-btn-clawdhub">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"/><line x1="2" y1="12" x2="22" y2="12"/></svg>
                  View on ClawdHub
                </a>
                <a href="https://raw.githubusercontent.com/openclaw/skills/main/skills/xsir0/google-gemini-media/SKILL.md" download="SKILL.md" class="sidebar-btn sidebar-btn-secondary">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"/><polyline points="7 10 12 15 17 10"/><line x1="12" y1="15" x2="12" y2="3"/></svg>
                  Download SKILL.md
                </a>
                <button onclick="sendToSecurityAuditor()" class="sidebar-btn sidebar-btn-security">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 22s8-4 8-10V5l-8-3-8 3v7c0 6 8 10 8 10z"/></svg>
                  Security Check
                </button>
              </div>
              <div class="sidebar-card">
                <div class="sidebar-title">Details</div>
                <div class="sidebar-info">
                  <div class="sidebar-info-item">
                    <span class="sidebar-info-label">Author</span>
                    <span class="sidebar-info-value">@xsir0</span>
                  </div>
                  <div class="sidebar-info-item">
                    <span class="sidebar-info-label">Category</span>
                    <span class="sidebar-info-value">Image & Video Generation</span>
                  </div>
                </div>
              </div>
            </aside>
          </div>
        
  <footer class="footer">
    <div class="footer-inner">
      <p class="footer-text" style="margin-bottom: 8px;"><a href="https://github.com/neonone123/moltdirectory" target="_blank" rel="noopener" style="color: var(--accent); text-decoration: none;">View on GitHub</a></p>
      <p class="footer-text" style="opacity: 0.6; font-size: 13px;">MoltDirectory.com is a community-run project and is not affiliated with the official MoltBot team or Peter Steinberger. We are just fans of the lobster.</p>
    </div>
  </footer>
  <script>
    function toggleTheme() {
      const html = document.documentElement;
      const currentTheme = html.getAttribute('data-theme');
      const newTheme = currentTheme === 'light' ? 'dark' : 'light';
      if (newTheme === 'light') {
        html.setAttribute('data-theme', 'light');
      } else {
        html.removeAttribute('data-theme');
      }
      localStorage.setItem('theme', newTheme);
    }
    
    function copySkillContent(btn) {
      const wrapper = btn.closest('.skill-source-wrapper');
      const content = wrapper.querySelector('.markdown-content');
      if (content) {
        const text = content.innerText || content.textContent;
        navigator.clipboard.writeText(text).then(() => {
          btn.classList.add('copied');
          btn.innerHTML = '<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="20 6 9 17 4 12"/></svg> Copied!';
          setTimeout(() => {
            btn.classList.remove('copied');
            btn.innerHTML = '<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="9" y="9" width="13" height="13" rx="2" ry="2"/><path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"/></svg> Copy';
          }, 2000);
        });
      }
    }

    async function sendToSecurityAuditor() {
      try {
        const contentElement = document.querySelector('.markdown-content');
        if (!contentElement) throw new Error('Could not find markdown content');
        const skillContent = contentElement.innerText || contentElement.textContent;
        localStorage.setItem('skillAuditContent', skillContent);
        window.open('/security-auditor', '_blank'); 
      } catch (error) {
        alert('Error: ' + error.message);
      }
    }
  </script>
</body>
</html>