<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Data engineering skill for building scalable">
  <title>senior-data-engineer - OpenClaw Directory</title>
  <link rel="canonical" href="https://moltdirectory.com/data-analytics/senior-data-engineer/">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>ü¶û</text></svg>">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/style.css">
  <script>
    // Apply saved theme before page renders to prevent flash
    (function() {
      const savedTheme = localStorage.getItem('theme') || 'dark';
      if (savedTheme === 'light') {
        document.documentElement.setAttribute('data-theme', 'light');
      }
    })();
  </script>
</head>
<body>
  <header class="header">
    <div class="header-inner">
      <a href="/" class="logo">

        <span class="logo-text">OpenClaw Directory</span>
      </a>
      <nav class="header-links">
        <a href="/start-here/" class="header-link">Start Here</a>
        <a href="/security-auditor" class="header-link">Security Auditor</a>
        <a href="https://github.com/neonone123/moltdirectory/issues/new?template=add-skill.yml" class="header-link" target="_blank" rel="noopener">Add Skill</a>
        <button class="theme-toggle" aria-label="Toggle theme" onclick="toggleTheme()">
          <svg class="icon-moon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/></svg>
          <svg class="icon-sun" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
        </button>
        <a href="https://www.reddit.com/r/OpenClawDirectory/" class="header-link" target="_blank" rel="noopener" aria-label="Community">
          <svg viewBox="0 0 16 16" fill="currentColor" width="28" height="28"><path d="M6.167 8a.83.83 0 0 0-.83.83c0 .459.372.84.83.831a.831.831 0 0 0 0-1.661m1.843 3.647c.315 0 1.403-.038 1.976-.611a.23.23 0 0 0 0-.306.213.213 0 0 0-.306 0c-.353.363-1.126.487-1.67.487-.545 0-1.308-.124-1.671-.487a.213.213 0 0 0-.306 0 .213.213 0 0 0 0 .306c.564.563 1.652.61 1.977.61zm.992-2.807c0 .458.373.83.831.83s.83-.381.83-.83a.831.831 0 0 0-1.66 0z"/><path d="M16 8A8 8 0 1 1 0 8a8 8 0 0 1 16 0m-3.828-1.165c-.315 0-.602.124-.812.325-.801-.573-1.9-.945-3.121-.993l.534-2.501 1.738.372a.83.83 0 1 0 .83-.869.83.83 0 0 0-.744.468l-1.938-.41a.2.2 0 0 0-.153.028.2.2 0 0 0-.086.134l-.592 2.788c-1.24.038-2.358.41-3.17.992-.21-.2-.496-.324-.81-.324a1.163 1.163 0 0 0-.478 2.224q-.03.17-.029.353c0 1.795 2.091 3.256 4.669 3.256s4.668-1.451 4.668-3.256c0-.114-.01-.238-.029-.353.401-.181.688-.592.688-1.069 0-.65-.525-1.165-1.165-1.165"/></svg>
        </a>
      </nav>
    </div>
  </header>
  
          <section class="skill-page-header">
            <div class="skill-page-header-inner">
              <a href="/data-analytics/" class="back-link">‚Üê Back to Data & Analytics</a>
              <div class="skill-page-meta">
                <a href="/data-analytics/" class="skill-page-category">Data & Analytics</a>
                <span class="skill-page-author">by @alirezarezvani</span>
              </div>
              <h1 class="skill-page-title">senior-data-engineer</h1>
              <p class="skill-page-desc">Data engineering skill for building scalable</p>
              <div class="vote-widget skill-page-vote" data-category-id="data-analytics" data-tool-id="senior-data-engineer">
                <button type="button" class="vote-btn" data-vote="1" aria-label="Upvote senior-data-engineer">‚ñ≤</button>
                <span class="vote-count">New</span>
                <button type="button" class="vote-btn" data-vote="-1" aria-label="Downvote senior-data-engineer">‚ñº</button>
              </div>
            </div>
          </section>
          <div class="skill-page-body">
            <article class="skill-page-content">
              <div class="skill-source-wrapper">
                <div class="skill-source-header">
                  <div class="skill-source-title">Source Code</div>
                  <button class="copy-btn" onclick="copySkillContent(this)">
                    <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="9" y="9" width="13" height="13" rx="2" ry="2"/><path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"/></svg>
                    Copy
                  </button>
                </div>
                <div class="markdown-content">
                  <h1>Senior Data Engineer</h1>
<p>Production-grade data engineering skill for building scalable, reliable data systems.</p>
<h2>Table of Contents</h2>
<ol>
<li><a href="#trigger-phrases">Trigger Phrases</a></li>
<li><a href="#quick-start">Quick Start</a></li>
<li><a href="#workflows">Workflows</a><ul>
<li><a href="#workflow-1-building-a-batch-etl-pipeline">Building a Batch ETL Pipeline</a></li>
<li><a href="#workflow-2-implementing-real-time-streaming">Implementing Real-Time Streaming</a></li>
<li><a href="#workflow-3-data-quality-framework-setup">Data Quality Framework Setup</a></li>
</ul>
</li>
<li><a href="#architecture-decision-framework">Architecture Decision Framework</a></li>
<li><a href="#tech-stack">Tech Stack</a></li>
<li><a href="#reference-documentation">Reference Documentation</a></li>
<li><a href="#troubleshooting">Troubleshooting</a></li>
</ol>
<hr>
<h2>Trigger Phrases</h2>
<p>Activate this skill when you see:</p>
<p><strong>Pipeline Design:</strong></p>
<ul>
<li>&quot;Design a data pipeline for...&quot;</li>
<li>&quot;Build an ETL/ELT process...&quot;</li>
<li>&quot;How should I ingest data from...&quot;</li>
<li>&quot;Set up data extraction from...&quot;</li>
</ul>
<p><strong>Architecture:</strong></p>
<ul>
<li>&quot;Should I use batch or streaming?&quot;</li>
<li>&quot;Lambda vs Kappa architecture&quot;</li>
<li>&quot;How to handle late-arriving data&quot;</li>
<li>&quot;Design a data lakehouse&quot;</li>
</ul>
<p><strong>Data Modeling:</strong></p>
<ul>
<li>&quot;Create a dimensional model...&quot;</li>
<li>&quot;Star schema vs snowflake&quot;</li>
<li>&quot;Implement slowly changing dimensions&quot;</li>
<li>&quot;Design a data vault&quot;</li>
</ul>
<p><strong>Data Quality:</strong></p>
<ul>
<li>&quot;Add data validation to...&quot;</li>
<li>&quot;Set up data quality checks&quot;</li>
<li>&quot;Monitor data freshness&quot;</li>
<li>&quot;Implement data contracts&quot;</li>
</ul>
<p><strong>Performance:</strong></p>
<ul>
<li>&quot;Optimize this Spark job&quot;</li>
<li>&quot;Query is running slow&quot;</li>
<li>&quot;Reduce pipeline execution time&quot;</li>
<li>&quot;Tune Airflow DAG&quot;</li>
</ul>
<hr>
<h2>Quick Start</h2>
<h3>Core Tools</h3>
<pre><code class="language-bash"># Generate pipeline orchestration config
python scripts/pipeline_orchestrator.py generate \
  --type airflow \
  --source postgres \
  --destination snowflake \
  --schedule &quot;0 5 * * *&quot;

# Validate data quality
python scripts/data_quality_validator.py validate \
  --input data/sales.parquet \
  --schema schemas/sales.json \
  --checks freshness,completeness,uniqueness

# Optimize ETL performance
python scripts/etl_performance_optimizer.py analyze \
  --query queries/daily_aggregation.sql \
  --engine spark \
  --recommend
</code></pre>
<hr>
<h2>Workflows</h2>
<h3>Workflow 1: Building a Batch ETL Pipeline</h3>
<p><strong>Scenario:</strong> Extract data from PostgreSQL, transform with dbt, load to Snowflake.</p>
<h4>Step 1: Define Source Schema</h4>
<pre><code class="language-sql">-- Document source tables
SELECT
    table_name,
    column_name,
    data_type,
    is_nullable
FROM information_schema.columns
WHERE table_schema = &#39;source_schema&#39;
ORDER BY table_name, ordinal_position;
</code></pre>
<h4>Step 2: Generate Extraction Config</h4>
<pre><code class="language-bash">python scripts/pipeline_orchestrator.py generate \
  --type airflow \
  --source postgres \
  --tables orders,customers,products \
  --mode incremental \
  --watermark updated_at \
  --output dags/extract_source.py
</code></pre>
<h4>Step 3: Create dbt Models</h4>
<pre><code class="language-sql">-- models/staging/stg_orders.sql
WITH source AS (
    SELECT * FROM {{ source(&#39;postgres&#39;, &#39;orders&#39;) }}
),

renamed AS (
    SELECT
        order_id,
        customer_id,
        order_date,
        total_amount,
        status,
        _extracted_at
    FROM source
    WHERE order_date &gt;= DATEADD(day, -3, CURRENT_DATE)
)

SELECT * FROM renamed
</code></pre>
<pre><code class="language-sql">-- models/marts/fct_orders.sql
{{
    config(
        materialized=&#39;incremental&#39;,
        unique_key=&#39;order_id&#39;,
        cluster_by=[&#39;order_date&#39;]
    )
}}

SELECT
    o.order_id,
    o.customer_id,
    c.customer_segment,
    o.order_date,
    o.total_amount,
    o.status
FROM {{ ref(&#39;stg_orders&#39;) }} o
LEFT JOIN {{ ref(&#39;dim_customers&#39;) }} c
    ON o.customer_id = c.customer_id

{% if is_incremental() %}
WHERE o._extracted_at &gt; (SELECT MAX(_extracted_at) FROM {{ this }})
{% endif %}
</code></pre>
<h4>Step 4: Configure Data Quality Tests</h4>
<pre><code class="language-yaml"># models/marts/schema.yml
version: 2

models:
  - name: fct_orders
    description: &quot;Order fact table&quot;
    columns:
      - name: order_id
        tests:
          - unique
          - not_null
      - name: total_amount
        tests:
          - not_null
          - dbt_utils.accepted_range:
              min_value: 0
              max_value: 1000000
      - name: order_date
        tests:
          - not_null
          - dbt_utils.recency:
              datepart: day
              field: order_date
              interval: 1
</code></pre>
<h4>Step 5: Create Airflow DAG</h4>
<pre><code class="language-python"># dags/daily_etl.py
from airflow import DAG
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago
from datetime import timedelta

default_args = {
    &#39;owner&#39;: &#39;data-team&#39;,
    &#39;depends_on_past&#39;: False,
    &#39;email_on_failure&#39;: True,
    &#39;email&#39;: [&#39;data-alerts@company.com&#39;],
    &#39;retries&#39;: 2,
    &#39;retry_delay&#39;: timedelta(minutes=5),
}

with DAG(
    &#39;daily_etl_pipeline&#39;,
    default_args=default_args,
    description=&#39;Daily ETL from PostgreSQL to Snowflake&#39;,
    schedule_interval=&#39;0 5 * * *&#39;,
    start_date=days_ago(1),
    catchup=False,
    tags=[&#39;etl&#39;, &#39;daily&#39;],
) as dag:

    extract = BashOperator(
        task_id=&#39;extract_source_data&#39;,
        bash_command=&#39;python /opt/airflow/scripts/extract.py --date {{ ds }}&#39;,
    )

    transform = BashOperator(
        task_id=&#39;run_dbt_models&#39;,
        bash_command=&#39;cd /opt/airflow/dbt &amp;&amp; dbt run --select marts.*&#39;,
    )

    test = BashOperator(
        task_id=&#39;run_dbt_tests&#39;,
        bash_command=&#39;cd /opt/airflow/dbt &amp;&amp; dbt test --select marts.*&#39;,
    )

    notify = BashOperator(
        task_id=&#39;send_notification&#39;,
        bash_command=&#39;python /opt/airflow/scripts/notify.py --status success&#39;,
        trigger_rule=&#39;all_success&#39;,
    )

    extract &gt;&gt; transform &gt;&gt; test &gt;&gt; notify
</code></pre>
<h4>Step 6: Validate Pipeline</h4>
<pre><code class="language-bash"># Test locally
dbt run --select stg_orders fct_orders
dbt test --select fct_orders

# Validate data quality
python scripts/data_quality_validator.py validate \
  --table fct_orders \
  --checks all \
  --output reports/quality_report.json
</code></pre>
<hr>
<h3>Workflow 2: Implementing Real-Time Streaming</h3>
<p><strong>Scenario:</strong> Stream events from Kafka, process with Flink/Spark Streaming, sink to data lake.</p>
<h4>Step 1: Define Event Schema</h4>
<pre><code class="language-json">{
  &quot;$schema&quot;: &quot;http://json-schema.org/draft-07/schema#&quot;,
  &quot;title&quot;: &quot;UserEvent&quot;,
  &quot;type&quot;: &quot;object&quot;,
  &quot;required&quot;: [&quot;event_id&quot;, &quot;user_id&quot;, &quot;event_type&quot;, &quot;timestamp&quot;],
  &quot;properties&quot;: {
    &quot;event_id&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;format&quot;: &quot;uuid&quot;},
    &quot;user_id&quot;: {&quot;type&quot;: &quot;string&quot;},
    &quot;event_type&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page_view&quot;, &quot;click&quot;, &quot;purchase&quot;]},
    &quot;timestamp&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;format&quot;: &quot;date-time&quot;},
    &quot;properties&quot;: {&quot;type&quot;: &quot;object&quot;}
  }
}
</code></pre>
<h4>Step 2: Create Kafka Topic</h4>
<pre><code class="language-bash"># Create topic with appropriate partitions
kafka-topics.sh --create \
  --bootstrap-server localhost:9092 \
  --topic user-events \
  --partitions 12 \
  --replication-factor 3 \
  --config retention.ms=604800000 \
  --config cleanup.policy=delete

# Verify topic
kafka-topics.sh --describe \
  --bootstrap-server localhost:9092 \
  --topic user-events
</code></pre>
<h4>Step 3: Implement Spark Streaming Job</h4>
<pre><code class="language-python"># streaming/user_events_processor.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    from_json, col, window, count, avg,
    to_timestamp, current_timestamp
)
from pyspark.sql.types import (
    StructType, StructField, StringType,
    TimestampType, MapType
)

# Initialize Spark
spark = SparkSession.builder \
    .appName(&quot;UserEventsProcessor&quot;) \
    .config(&quot;spark.sql.streaming.checkpointLocation&quot;, &quot;/checkpoints/user-events&quot;) \
    .config(&quot;spark.sql.shuffle.partitions&quot;, &quot;12&quot;) \
    .getOrCreate()

# Define schema
event_schema = StructType([
    StructField(&quot;event_id&quot;, StringType(), False),
    StructField(&quot;user_id&quot;, StringType(), False),
    StructField(&quot;event_type&quot;, StringType(), False),
    StructField(&quot;timestamp&quot;, StringType(), False),
    StructField(&quot;properties&quot;, MapType(StringType(), StringType()), True)
])

# Read from Kafka
events_df = spark.readStream \
    .format(&quot;kafka&quot;) \
    .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;) \
    .option(&quot;subscribe&quot;, &quot;user-events&quot;) \
    .option(&quot;startingOffsets&quot;, &quot;latest&quot;) \
    .option(&quot;failOnDataLoss&quot;, &quot;false&quot;) \
    .load()

# Parse JSON
parsed_df = events_df \
    .select(from_json(col(&quot;value&quot;).cast(&quot;string&quot;), event_schema).alias(&quot;data&quot;)) \
    .select(&quot;data.*&quot;) \
    .withColumn(&quot;event_timestamp&quot;, to_timestamp(col(&quot;timestamp&quot;)))

# Windowed aggregation
aggregated_df = parsed_df \
    .withWatermark(&quot;event_timestamp&quot;, &quot;10 minutes&quot;) \
    .groupBy(
        window(col(&quot;event_timestamp&quot;), &quot;5 minutes&quot;),
        col(&quot;event_type&quot;)
    ) \
    .agg(
        count(&quot;*&quot;).alias(&quot;event_count&quot;),
        approx_count_distinct(&quot;user_id&quot;).alias(&quot;unique_users&quot;)
    )

# Write to Delta Lake
query = aggregated_df.writeStream \
    .format(&quot;delta&quot;) \
    .outputMode(&quot;append&quot;) \
    .option(&quot;checkpointLocation&quot;, &quot;/checkpoints/user-events-aggregated&quot;) \
    .option(&quot;path&quot;, &quot;/data/lake/user_events_aggregated&quot;) \
    .trigger(processingTime=&quot;1 minute&quot;) \
    .start()

query.awaitTermination()
</code></pre>
<h4>Step 4: Handle Late Data and Errors</h4>
<pre><code class="language-python"># Dead letter queue for failed records
from pyspark.sql.functions import current_timestamp, lit

def process_with_error_handling(batch_df, batch_id):
    try:
        # Attempt processing
        valid_df = batch_df.filter(col(&quot;event_id&quot;).isNotNull())
        invalid_df = batch_df.filter(col(&quot;event_id&quot;).isNull())

        # Write valid records
        valid_df.write \
            .format(&quot;delta&quot;) \
            .mode(&quot;append&quot;) \
            .save(&quot;/data/lake/user_events&quot;)

        # Write invalid to DLQ
        if invalid_df.count() &gt; 0:
            invalid_df \
                .withColumn(&quot;error_timestamp&quot;, current_timestamp()) \
                .withColumn(&quot;error_reason&quot;, lit(&quot;missing_event_id&quot;)) \
                .write \
                .format(&quot;delta&quot;) \
                .mode(&quot;append&quot;) \
                .save(&quot;/data/lake/dlq/user_events&quot;)

    except Exception as e:
        # Log error, alert, continue
        logger.error(f&quot;Batch {batch_id} failed: {e}&quot;)
        raise

# Use foreachBatch for custom processing
query = parsed_df.writeStream \
    .foreachBatch(process_with_error_handling) \
    .option(&quot;checkpointLocation&quot;, &quot;/checkpoints/user-events&quot;) \
    .start()
</code></pre>
<h4>Step 5: Monitor Stream Health</h4>
<pre><code class="language-python"># monitoring/stream_metrics.py
from prometheus_client import Gauge, Counter, start_http_server

# Define metrics
RECORDS_PROCESSED = Counter(
    &#39;stream_records_processed_total&#39;,
    &#39;Total records processed&#39;,
    [&#39;stream_name&#39;, &#39;status&#39;]
)

PROCESSING_LAG = Gauge(
    &#39;stream_processing_lag_seconds&#39;,
    &#39;Current processing lag&#39;,
    [&#39;stream_name&#39;]
)

BATCH_DURATION = Gauge(
    &#39;stream_batch_duration_seconds&#39;,
    &#39;Last batch processing duration&#39;,
    [&#39;stream_name&#39;]
)

def emit_metrics(query):
    &quot;&quot;&quot;Emit Prometheus metrics from streaming query.&quot;&quot;&quot;
    progress = query.lastProgress
    if progress:
        RECORDS_PROCESSED.labels(
            stream_name=&#39;user-events&#39;,
            status=&#39;success&#39;
        ).inc(progress[&#39;numInputRows&#39;])

        if progress[&#39;sources&#39;]:
            # Calculate lag from latest offset
            for source in progress[&#39;sources&#39;]:
                end_offset = source.get(&#39;endOffset&#39;, {})
                # Parse Kafka offsets and calculate lag
</code></pre>
<hr>
<h3>Workflow 3: Data Quality Framework Setup</h3>
<p><strong>Scenario:</strong> Implement comprehensive data quality monitoring with Great Expectations.</p>
<h4>Step 1: Initialize Great Expectations</h4>
<pre><code class="language-bash"># Install and initialize
pip install great_expectations

great_expectations init

# Connect to data source
great_expectations datasource new
</code></pre>
<h4>Step 2: Create Expectation Suite</h4>
<pre><code class="language-python"># expectations/orders_suite.py
import great_expectations as gx

context = gx.get_context()

# Create expectation suite
suite = context.add_expectation_suite(&quot;orders_quality_suite&quot;)

# Add expectations
validator = context.get_validator(
    batch_request={
        &quot;datasource_name&quot;: &quot;warehouse&quot;,
        &quot;data_asset_name&quot;: &quot;orders&quot;,
    },
    expectation_suite_name=&quot;orders_quality_suite&quot;
)

# Schema expectations
validator.expect_table_columns_to_match_ordered_list(
    column_list=[
        &quot;order_id&quot;, &quot;customer_id&quot;, &quot;order_date&quot;,
        &quot;total_amount&quot;, &quot;status&quot;, &quot;created_at&quot;
    ]
)

# Completeness expectations
validator.expect_column_values_to_not_be_null(&quot;order_id&quot;)
validator.expect_column_values_to_not_be_null(&quot;customer_id&quot;)
validator.expect_column_values_to_not_be_null(&quot;order_date&quot;)

# Uniqueness expectations
validator.expect_column_values_to_be_unique(&quot;order_id&quot;)

# Range expectations
validator.expect_column_values_to_be_between(
    &quot;total_amount&quot;,
    min_value=0,
    max_value=1000000
)

# Categorical expectations
validator.expect_column_values_to_be_in_set(
    &quot;status&quot;,
    [&quot;pending&quot;, &quot;confirmed&quot;, &quot;shipped&quot;, &quot;delivered&quot;, &quot;cancelled&quot;]
)

# Freshness expectation
validator.expect_column_max_to_be_between(
    &quot;order_date&quot;,
    min_value={&quot;$PARAMETER&quot;: &quot;now - timedelta(days=1)&quot;},
    max_value={&quot;$PARAMETER&quot;: &quot;now&quot;}
)

# Referential integrity
validator.expect_column_values_to_be_in_set(
    &quot;customer_id&quot;,
    value_set={&quot;$PARAMETER&quot;: &quot;valid_customer_ids&quot;}
)

validator.save_expectation_suite(discard_failed_expectations=False)
</code></pre>
<h4>Step 3: Create Data Quality Checks with dbt</h4>
<pre><code class="language-yaml"># models/marts/schema.yml
version: 2

models:
  - name: fct_orders
    description: &quot;Order fact table with data quality checks&quot;

    tests:
      # Row count check
      - dbt_utils.equal_rowcount:
          compare_model: ref(&#39;stg_orders&#39;)

      # Freshness check
      - dbt_utils.recency:
          datepart: hour
          field: created_at
          interval: 24

    columns:
      - name: order_id
        description: &quot;Unique order identifier&quot;
        tests:
          - unique
          - not_null
          - relationships:
              to: ref(&#39;dim_orders&#39;)
              field: order_id

      - name: total_amount
        tests:
          - not_null
          - dbt_utils.accepted_range:
              min_value: 0
              max_value: 1000000
              inclusive: true
          - dbt_expectations.expect_column_values_to_be_between:
              min_value: 0
              row_condition: &quot;status != &#39;cancelled&#39;&quot;

      - name: customer_id
        tests:
          - not_null
          - relationships:
              to: ref(&#39;dim_customers&#39;)
              field: customer_id
              severity: warn
</code></pre>
<h4>Step 4: Implement Data Contracts</h4>
<pre><code class="language-yaml"># contracts/orders_contract.yaml
contract:
  name: orders_data_contract
  version: &quot;1.0.0&quot;
  owner: data-team@company.com

schema:
  type: object
  properties:
    order_id:
      type: string
      format: uuid
      description: &quot;Unique order identifier&quot;
    customer_id:
      type: string
      not_null: true
    order_date:
      type: date
      not_null: true
    total_amount:
      type: decimal
      precision: 10
      scale: 2
      minimum: 0
    status:
      type: string
      enum: [&quot;pending&quot;, &quot;confirmed&quot;, &quot;shipped&quot;, &quot;delivered&quot;, &quot;cancelled&quot;]

sla:
  freshness:
    max_delay_hours: 1
  completeness:
    min_percentage: 99.9
  accuracy:
    duplicate_tolerance: 0.01

consumers:
  - name: analytics-team
    usage: &quot;Daily reporting dashboards&quot;
  - name: ml-team
    usage: &quot;Churn prediction model&quot;
</code></pre>
<h4>Step 5: Set Up Quality Monitoring Dashboard</h4>
<pre><code class="language-python"># monitoring/quality_dashboard.py
from datetime import datetime, timedelta
import pandas as pd

def generate_quality_report(connection, table_name: str) -&gt; dict:
    &quot;&quot;&quot;Generate comprehensive data quality report.&quot;&quot;&quot;

    report = {
        &quot;table&quot;: table_name,
        &quot;timestamp&quot;: datetime.now().isoformat(),
        &quot;checks&quot;: {}
    }

    # Row count check
    row_count = connection.execute(
        f&quot;SELECT COUNT(*) FROM {table_name}&quot;
    ).fetchone()[0]
    report[&quot;checks&quot;][&quot;row_count&quot;] = {
        &quot;value&quot;: row_count,
        &quot;status&quot;: &quot;pass&quot; if row_count &gt; 0 else &quot;fail&quot;
    }

    # Freshness check
    max_date = connection.execute(
        f&quot;SELECT MAX(created_at) FROM {table_name}&quot;
    ).fetchone()[0]
    hours_old = (datetime.now() - max_date).total_seconds() / 3600
    report[&quot;checks&quot;][&quot;freshness&quot;] = {
        &quot;max_timestamp&quot;: max_date.isoformat(),
        &quot;hours_old&quot;: round(hours_old, 2),
        &quot;status&quot;: &quot;pass&quot; if hours_old &lt; 24 else &quot;fail&quot;
    }

    # Null rate check
    null_query = f&quot;&quot;&quot;
    SELECT
        SUM(CASE WHEN order_id IS NULL THEN 1 ELSE 0 END) as null_order_id,
        SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) as null_customer_id,
        COUNT(*) as total
    FROM {table_name}
    &quot;&quot;&quot;
    null_result = connection.execute(null_query).fetchone()
    report[&quot;checks&quot;][&quot;null_rates&quot;] = {
        &quot;order_id&quot;: null_result[0] / null_result[2] if null_result[2] &gt; 0 else 0,
        &quot;customer_id&quot;: null_result[1] / null_result[2] if null_result[2] &gt; 0 else 0,
        &quot;status&quot;: &quot;pass&quot; if null_result[0] == 0 and null_result[1] == 0 else &quot;fail&quot;
    }

    # Duplicate check
    dup_query = f&quot;&quot;&quot;
    SELECT COUNT(*) - COUNT(DISTINCT order_id) as duplicates
    FROM {table_name}
    &quot;&quot;&quot;
    duplicates = connection.execute(dup_query).fetchone()[0]
    report[&quot;checks&quot;][&quot;duplicates&quot;] = {
        &quot;count&quot;: duplicates,
        &quot;status&quot;: &quot;pass&quot; if duplicates == 0 else &quot;fail&quot;
    }

    # Overall status
    all_passed = all(
        check[&quot;status&quot;] == &quot;pass&quot;
        for check in report[&quot;checks&quot;].values()
    )
    report[&quot;overall_status&quot;] = &quot;pass&quot; if all_passed else &quot;fail&quot;

    return report
</code></pre>
<hr>
<h2>Architecture Decision Framework</h2>
<p>Use this framework to choose the right approach for your data pipeline.</p>
<h3>Batch vs Streaming</h3>
<table>
<thead>
<tr>
<th>Criteria</th>
<th>Batch</th>
<th>Streaming</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Latency requirement</strong></td>
<td>Hours to days</td>
<td>Seconds to minutes</td>
</tr>
<tr>
<td><strong>Data volume</strong></td>
<td>Large historical datasets</td>
<td>Continuous event streams</td>
</tr>
<tr>
<td><strong>Processing complexity</strong></td>
<td>Complex transformations, ML</td>
<td>Simple aggregations, filtering</td>
</tr>
<tr>
<td><strong>Cost sensitivity</strong></td>
<td>More cost-effective</td>
<td>Higher infrastructure cost</td>
</tr>
<tr>
<td><strong>Error handling</strong></td>
<td>Easier to reprocess</td>
<td>Requires careful design</td>
</tr>
</tbody></table>
<p><strong>Decision Tree:</strong></p>
<pre><code>Is real-time insight required?
‚îú‚îÄ‚îÄ Yes ‚Üí Use streaming
‚îÇ   ‚îî‚îÄ‚îÄ Is exactly-once semantics needed?
‚îÇ       ‚îú‚îÄ‚îÄ Yes ‚Üí Kafka + Flink/Spark Structured Streaming
‚îÇ       ‚îî‚îÄ‚îÄ No ‚Üí Kafka + consumer groups
‚îî‚îÄ‚îÄ No ‚Üí Use batch
    ‚îî‚îÄ‚îÄ Is data volume &gt; 1TB daily?
        ‚îú‚îÄ‚îÄ Yes ‚Üí Spark/Databricks
        ‚îî‚îÄ‚îÄ No ‚Üí dbt + warehouse compute
</code></pre>
<h3>Lambda vs Kappa Architecture</h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Lambda</th>
<th>Kappa</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Complexity</strong></td>
<td>Two codebases (batch + stream)</td>
<td>Single codebase</td>
</tr>
<tr>
<td><strong>Maintenance</strong></td>
<td>Higher (sync batch/stream logic)</td>
<td>Lower</td>
</tr>
<tr>
<td><strong>Reprocessing</strong></td>
<td>Native batch layer</td>
<td>Replay from source</td>
</tr>
<tr>
<td><strong>Use case</strong></td>
<td>ML training + real-time serving</td>
<td>Pure event-driven</td>
</tr>
</tbody></table>
<p><strong>When to choose Lambda:</strong></p>
<ul>
<li>Need to train ML models on historical data</li>
<li>Complex batch transformations not feasible in streaming</li>
<li>Existing batch infrastructure</li>
</ul>
<p><strong>When to choose Kappa:</strong></p>
<ul>
<li>Event-sourced architecture</li>
<li>All processing can be expressed as stream operations</li>
<li>Starting fresh without legacy systems</li>
</ul>
<h3>Data Warehouse vs Data Lakehouse</h3>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Warehouse (Snowflake/BigQuery)</th>
<th>Lakehouse (Delta/Iceberg)</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Best for</strong></td>
<td>BI, SQL analytics</td>
<td>ML, unstructured data</td>
</tr>
<tr>
<td><strong>Storage cost</strong></td>
<td>Higher (proprietary format)</td>
<td>Lower (open formats)</td>
</tr>
<tr>
<td><strong>Flexibility</strong></td>
<td>Schema-on-write</td>
<td>Schema-on-read</td>
</tr>
<tr>
<td><strong>Performance</strong></td>
<td>Excellent for SQL</td>
<td>Good, improving</td>
</tr>
<tr>
<td><strong>Ecosystem</strong></td>
<td>Mature BI tools</td>
<td>Growing ML tooling</td>
</tr>
</tbody></table>
<hr>
<h2>Tech Stack</h2>
<table>
<thead>
<tr>
<th>Category</th>
<th>Technologies</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Languages</strong></td>
<td>Python, SQL, Scala</td>
</tr>
<tr>
<td><strong>Orchestration</strong></td>
<td>Airflow, Prefect, Dagster</td>
</tr>
<tr>
<td><strong>Transformation</strong></td>
<td>dbt, Spark, Flink</td>
</tr>
<tr>
<td><strong>Streaming</strong></td>
<td>Kafka, Kinesis, Pub/Sub</td>
</tr>
<tr>
<td><strong>Storage</strong></td>
<td>S3, GCS, Delta Lake, Iceberg</td>
</tr>
<tr>
<td><strong>Warehouses</strong></td>
<td>Snowflake, BigQuery, Redshift, Databricks</td>
</tr>
<tr>
<td><strong>Quality</strong></td>
<td>Great Expectations, dbt tests, Monte Carlo</td>
</tr>
<tr>
<td><strong>Monitoring</strong></td>
<td>Prometheus, Grafana, Datadog</td>
</tr>
</tbody></table>
<hr>
<h2>Reference Documentation</h2>
<h3>1. Data Pipeline Architecture</h3>
<p>See <code>references/data_pipeline_architecture.md</code> for:</p>
<ul>
<li>Lambda vs Kappa architecture patterns</li>
<li>Batch processing with Spark and Airflow</li>
<li>Stream processing with Kafka and Flink</li>
<li>Exactly-once semantics implementation</li>
<li>Error handling and dead letter queues</li>
</ul>
<h3>2. Data Modeling Patterns</h3>
<p>See <code>references/data_modeling_patterns.md</code> for:</p>
<ul>
<li>Dimensional modeling (Star/Snowflake)</li>
<li>Slowly Changing Dimensions (SCD Types 1-6)</li>
<li>Data Vault modeling</li>
<li>dbt best practices</li>
<li>Partitioning and clustering</li>
</ul>
<h3>3. DataOps Best Practices</h3>
<p>See <code>references/dataops_best_practices.md</code> for:</p>
<ul>
<li>Data testing frameworks</li>
<li>Data contracts and schema validation</li>
<li>CI/CD for data pipelines</li>
<li>Observability and lineage</li>
<li>Incident response</li>
</ul>
<hr>
<h2>Troubleshooting</h2>
<h3>Pipeline Failures</h3>
<p><strong>Symptom:</strong> Airflow DAG fails with timeout</p>
<pre><code>Task exceeded max execution time
</code></pre>
<p><strong>Solution:</strong></p>
<ol>
<li>Check resource allocation</li>
<li>Profile slow operations</li>
<li>Add incremental processing</li>
</ol>
<pre><code class="language-python"># Increase timeout
default_args = {
    &#39;execution_timeout&#39;: timedelta(hours=2),
}

# Or use incremental loads
WHERE updated_at &gt; &#39;{{ prev_ds }}&#39;
</code></pre>
<hr>
<p><strong>Symptom:</strong> Spark job OOM</p>
<pre><code>java.lang.OutOfMemoryError: Java heap space
</code></pre>
<p><strong>Solution:</strong></p>
<ol>
<li>Increase executor memory</li>
<li>Reduce partition size</li>
<li>Use disk spill</li>
</ol>
<pre><code class="language-python">spark.conf.set(&quot;spark.executor.memory&quot;, &quot;8g&quot;)
spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;, &quot;200&quot;)
spark.conf.set(&quot;spark.memory.fraction&quot;, &quot;0.8&quot;)
</code></pre>
<hr>
<p><strong>Symptom:</strong> Kafka consumer lag increasing</p>
<pre><code>Consumer lag: 1000000 messages
</code></pre>
<p><strong>Solution:</strong></p>
<ol>
<li>Increase consumer parallelism</li>
<li>Optimize processing logic</li>
<li>Scale consumer group</li>
</ol>
<pre><code class="language-bash"># Add more partitions
kafka-topics.sh --alter \
  --bootstrap-server localhost:9092 \
  --topic user-events \
  --partitions 24
</code></pre>
<hr>
<h3>Data Quality Issues</h3>
<p><strong>Symptom:</strong> Duplicate records appearing</p>
<pre><code>Expected unique, found 150 duplicates
</code></pre>
<p><strong>Solution:</strong></p>
<ol>
<li>Add deduplication logic</li>
<li>Use merge/upsert operations</li>
</ol>
<pre><code class="language-sql">-- dbt incremental with dedup
{{
    config(
        materialized=&#39;incremental&#39;,
        unique_key=&#39;order_id&#39;
    )
}}

SELECT * FROM (
    SELECT
        *,
        ROW_NUMBER() OVER (
            PARTITION BY order_id
            ORDER BY updated_at DESC
        ) as rn
    FROM {{ source(&#39;raw&#39;, &#39;orders&#39;) }}
) WHERE rn = 1
</code></pre>
<hr>
<p><strong>Symptom:</strong> Stale data in tables</p>
<pre><code>Last update: 3 days ago
</code></pre>
<p><strong>Solution:</strong></p>
<ol>
<li>Check upstream pipeline status</li>
<li>Verify source availability</li>
<li>Add freshness monitoring</li>
</ol>
<pre><code class="language-yaml"># dbt freshness check
sources:
  - name: raw
    freshness:
      warn_after: {count: 12, period: hour}
      error_after: {count: 24, period: hour}
    loaded_at_field: _loaded_at
</code></pre>
<hr>
<p><strong>Symptom:</strong> Schema drift detected</p>
<pre><code>Column &#39;new_field&#39; not in expected schema
</code></pre>
<p><strong>Solution:</strong></p>
<ol>
<li>Update data contract</li>
<li>Modify transformations</li>
<li>Communicate with producers</li>
</ol>
<pre><code class="language-python"># Handle schema evolution
df = spark.read.format(&quot;delta&quot;) \
    .option(&quot;mergeSchema&quot;, &quot;true&quot;) \
    .load(&quot;/data/orders&quot;)
</code></pre>
<hr>
<h3>Performance Issues</h3>
<p><strong>Symptom:</strong> Query takes hours</p>
<pre><code>Query runtime: 4 hours (expected: 30 minutes)
</code></pre>
<p><strong>Solution:</strong></p>
<ol>
<li>Check query plan</li>
<li>Add proper partitioning</li>
<li>Optimize joins</li>
</ol>
<pre><code class="language-sql">-- Before: Full table scan
SELECT * FROM orders WHERE order_date = &#39;2024-01-15&#39;;

-- After: Partition pruning
-- Table partitioned by order_date
SELECT * FROM orders WHERE order_date = &#39;2024-01-15&#39;;

-- Add clustering for frequent filters
ALTER TABLE orders CLUSTER BY (customer_id);
</code></pre>
<hr>
<p><strong>Symptom:</strong> dbt model takes too long</p>
<pre><code>Model fct_orders completed in 45 minutes
</code></pre>
<p><strong>Solution:</strong></p>
<ol>
<li>Use incremental materialization</li>
<li>Reduce upstream dependencies</li>
<li>Pre-aggregate where possible</li>
</ol>
<pre><code class="language-sql">-- Convert to incremental
{{
    config(
        materialized=&#39;incremental&#39;,
        unique_key=&#39;order_id&#39;,
        on_schema_change=&#39;sync_all_columns&#39;
    )
}}

SELECT * FROM {{ ref(&#39;stg_orders&#39;) }}
{% if is_incremental() %}
WHERE _loaded_at &gt; (SELECT MAX(_loaded_at) FROM {{ this }})
{% endif %}
</code></pre>

                </div>
              </div>
            </article>
            <aside class="skill-page-sidebar">
              <div class="sidebar-card">
                <div class="sidebar-title">Actions</div>
                <a href="https://github.com/openclaw/skills/tree/main/skills/alirezarezvani/senior-data-engineer/SKILL.md" target="_blank" rel="noopener" class="sidebar-btn sidebar-btn-primary">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/></svg>
                  View on GitHub
                </a>
                <a href="https://clawdhub.com/alirezarezvani/senior-data-engineer" target="_blank" rel="noopener" class="sidebar-btn sidebar-btn-clawdhub">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"/><line x1="2" y1="12" x2="22" y2="12"/></svg>
                  View on ClawdHub
                </a>
                <a href="https://raw.githubusercontent.com/openclaw/skills/main/skills/alirezarezvani/senior-data-engineer/SKILL.md" download="SKILL.md" class="sidebar-btn sidebar-btn-secondary">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"/><polyline points="7 10 12 15 17 10"/><line x1="12" y1="15" x2="12" y2="3"/></svg>
                  Download SKILL.md
                </a>
                <button onclick="sendToSecurityAuditor()" class="sidebar-btn sidebar-btn-security">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 22s8-4 8-10V5l-8-3-8 3v7c0 6 8 10 8 10z"/></svg>
                  Security Check
                </button>
              </div>
              <div class="sidebar-card">
                <div class="sidebar-title">Details</div>
                <div class="sidebar-info">
                  <div class="sidebar-info-item">
                    <span class="sidebar-info-label">Author</span>
                    <span class="sidebar-info-value">@alirezarezvani</span>
                  </div>
                  <div class="sidebar-info-item">
                    <span class="sidebar-info-label">Category</span>
                    <span class="sidebar-info-value">Data & Analytics</span>
                  </div>
                </div>
              </div>
            </aside>
          </div>
        
  <footer class="footer">
    <div class="footer-inner">
      <p class="footer-text" style="margin-bottom: 8px;"><a href="https://github.com/neonone123/moltdirectory" target="_blank" rel="noopener" style="color: var(--accent); text-decoration: none;">View on GitHub</a></p>
      <p class="footer-text" style="opacity: 0.6; font-size: 13px;">OpenClawDirectory.com is a community-run project and is not affiliated with the official OpenClaw team or Peter Steinberger. We are just fans of the lobster.</p>
    </div>
  </footer>
  <script>
    function toggleTheme() {
      const html = document.documentElement;
      const currentTheme = html.getAttribute('data-theme');
      const newTheme = currentTheme === 'light' ? 'dark' : 'light';
      if (newTheme === 'light') {
        html.setAttribute('data-theme', 'light');
      } else {
        html.removeAttribute('data-theme');
      }
      localStorage.setItem('theme', newTheme);
    }
    
    function copySkillContent(btn) {
      const wrapper = btn.closest('.skill-source-wrapper');
      const content = wrapper.querySelector('.markdown-content');
      if (content) {
        const text = content.innerText || content.textContent;
        navigator.clipboard.writeText(text).then(() => {
          btn.classList.add('copied');
          btn.innerHTML = '<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="20 6 9 17 4 12"/></svg> Copied!';
          setTimeout(() => {
            btn.classList.remove('copied');
            btn.innerHTML = '<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="9" y="9" width="13" height="13" rx="2" ry="2"/><path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"/></svg> Copy';
          }, 2000);
        });
      }
    }

    async function sendToSecurityAuditor() {
      try {
        const contentElement = document.querySelector('.markdown-content');
        if (!contentElement) throw new Error('Could not find markdown content');
        const skillContent = contentElement.innerText || contentElement.textContent;
        localStorage.setItem('skillAuditContent', skillContent);
        window.open('/security-auditor', '_blank'); 
      } catch (error) {
        alert('Error: ' + error.message);
      }
    }

    function updateVoteWidget(widget, toolData) {
      if (!widget || !toolData) return;
      const upBtn = widget.querySelector('[data-vote="1"]');
      const downBtn = widget.querySelector('[data-vote="-1"]');
      const countNode = widget.querySelector('.vote-count');
      widget.dataset.rankScore = String(toolData.rankScore ?? 0);
      widget.dataset.viewerVote = String(toolData.viewerVote ?? 0);

      if (countNode) {
        countNode.textContent = toolData.displayCount || 'New';
      }
      if (upBtn) {
        upBtn.classList.toggle('is-active', Number(toolData.viewerVote) === 1);
      }
      if (downBtn) {
        downBtn.classList.toggle('is-active', Number(toolData.viewerVote) === -1);
      }
    }

    function setWidgetBusy(widget, busy) {
      widget.dataset.busy = busy ? '1' : '0';
      widget.querySelectorAll('.vote-btn').forEach((btn) => {
        btn.disabled = busy;
      });
    }

    function applySortMode(grid, mode) {
      const cards = [...grid.querySelectorAll('.skill-card[data-tool-id]')];
      cards.sort((a, b) => {
        if (mode === 'community') {
          const scoreDiff = Number(b.dataset.rankScore || 0) - Number(a.dataset.rankScore || 0);
          if (scoreDiff !== 0) return scoreDiff;
        }
        return Number(a.dataset.originalIndex || 0) - Number(b.dataset.originalIndex || 0);
      });
      cards.forEach((card) => grid.appendChild(card));
    }

    function bindSortToggle(toolbar, grid) {
      const buttons = toolbar.querySelectorAll('.sort-toggle-btn');
      const status = toolbar.querySelector('.sort-status');
      buttons.forEach((btn) => {
        btn.addEventListener('click', () => {
          const mode = btn.dataset.sortMode;
          buttons.forEach((b) => b.classList.toggle('is-active', b === btn));
          applySortMode(grid, mode);
          status.textContent = mode === 'community' ? 'Community ranking active' : 'Original order active';
        });
      });
    }

    async function fetchScores(categoryId, toolIds) {
      const params = new URLSearchParams({
        categoryId,
        toolIds: toolIds.join(',')
      });
      const response = await fetch('/api/v1/scores?' + params.toString(), {
        method: 'GET',
        credentials: 'same-origin'
      });
      if (!response.ok) {
        throw new Error('Unable to fetch vote scores');
      }
      return response.json();
    }

    async function submitVote(widget, voteValue) {
      const categoryId = widget.dataset.categoryId;
      const toolId = widget.dataset.toolId;
      setWidgetBusy(widget, true);
      try {
        const response = await fetch('/api/v1/vote', {
          method: 'POST',
          credentials: 'same-origin',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            categoryId,
            toolId,
            vote: voteValue
          })
        });
        const payload = await response.json().catch(() => ({}));
        if (!response.ok) {
          throw new Error(payload.error || 'Vote request failed');
        }
        if (payload.tool) {
          updateVoteWidget(widget, payload.tool);
          const card = widget.closest('.skill-card[data-tool-id]');
          if (card) {
            card.dataset.rankScore = String(payload.tool.rankScore ?? 0);
          }
        }
      } catch (error) {
        console.error(error);
      } finally {
        setWidgetBusy(widget, false);
      }
    }

    document.addEventListener('DOMContentLoaded', async () => {
      const widgets = [...document.querySelectorAll('.vote-widget[data-category-id][data-tool-id]')];
      if (widgets.length === 0) return;

      const grouped = new Map();
      widgets.forEach((widget) => {
        const key = widget.dataset.categoryId;
        if (!grouped.has(key)) grouped.set(key, new Map());
        grouped.get(key).set(widget.dataset.toolId, widget);
      });

      for (const [categoryId, widgetMap] of grouped.entries()) {
        const toolIds = [...widgetMap.keys()];
        try {
          const data = await fetchScores(categoryId, toolIds);
          const byTool = new Map((data.tools || []).map((tool) => [tool.toolId, tool]));
          for (const [toolId, widget] of widgetMap.entries()) {
            const toolData = byTool.get(toolId);
            if (toolData) {
              updateVoteWidget(widget, toolData);
              const card = widget.closest('.skill-card[data-tool-id]');
              if (card) {
                card.dataset.rankScore = String(toolData.rankScore ?? 0);
              }
            }
          }
        } catch (error) {
          console.error(error);
        }
      }

      widgets.forEach((widget) => {
        widget.querySelectorAll('.vote-btn').forEach((btn) => {
          btn.addEventListener('click', (event) => {
            event.preventDefault();
            event.stopPropagation();
            if (widget.dataset.busy === '1') return;
            const voteValue = Number(btn.dataset.vote);
            submitVote(widget, voteValue).then(() => {
              const grid = widget.closest('.skills-grid[data-category-id]');
              const toolbar = document.querySelector('.sort-toolbar[data-category-id="' + widget.dataset.categoryId + '"]');
              const communityBtn = toolbar ? toolbar.querySelector('.sort-toggle-btn[data-sort-mode="community"]') : null;
              if (grid && communityBtn && communityBtn.classList.contains('is-active')) {
                applySortMode(grid, 'community');
              }
            });
          });
        });
      });

      document.querySelectorAll('.skills-grid[data-category-id]').forEach((grid) => {
        const categoryId = grid.dataset.categoryId;
        const toolbar = document.querySelector('.sort-toolbar[data-category-id="' + categoryId + '"]');
        if (!toolbar) return;
        bindSortToggle(toolbar, grid);
        applySortMode(grid, 'community');
      });
    });
  </script>
</body>
</html>