<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Build real-time voice AI applications">
  <title>azure-ai-voicelive-py - OpenClaw Directory</title>
  <link rel="canonical" href="https://moltdirectory.com/devops-cloud/azure-ai-voicelive-py/">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>ü¶û</text></svg>">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/style.css">
  <script>
    // Apply saved theme before page renders to prevent flash
    (function() {
      const savedTheme = localStorage.getItem('theme') || 'dark';
      if (savedTheme === 'light') {
        document.documentElement.setAttribute('data-theme', 'light');
      }
    })();
  </script>
</head>
<body>
  <header class="header">
    <div class="header-inner">
      <a href="/" class="logo">

        <span class="logo-text">OpenClaw Directory</span>
      </a>
      <nav class="header-links">
        <a href="/start-here/" class="header-link">Start Here</a>
        <a href="/security-auditor" class="header-link">Security Auditor</a>
        <a href="https://github.com/neonone123/moltdirectory/issues/new?template=add-skill.yml" class="header-link" target="_blank" rel="noopener">Add Skill</a>
        <button class="theme-toggle" aria-label="Toggle theme" onclick="toggleTheme()">
          <svg class="icon-moon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/></svg>
          <svg class="icon-sun" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
        </button>
        <a href="https://www.reddit.com/r/OpenClawDirectory/" class="header-link" target="_blank" rel="noopener" aria-label="Community">
          <svg viewBox="0 0 16 16" fill="currentColor" width="28" height="28"><path d="M6.167 8a.83.83 0 0 0-.83.83c0 .459.372.84.83.831a.831.831 0 0 0 0-1.661m1.843 3.647c.315 0 1.403-.038 1.976-.611a.23.23 0 0 0 0-.306.213.213 0 0 0-.306 0c-.353.363-1.126.487-1.67.487-.545 0-1.308-.124-1.671-.487a.213.213 0 0 0-.306 0 .213.213 0 0 0 0 .306c.564.563 1.652.61 1.977.61zm.992-2.807c0 .458.373.83.831.83s.83-.381.83-.83a.831.831 0 0 0-1.66 0z"/><path d="M16 8A8 8 0 1 1 0 8a8 8 0 0 1 16 0m-3.828-1.165c-.315 0-.602.124-.812.325-.801-.573-1.9-.945-3.121-.993l.534-2.501 1.738.372a.83.83 0 1 0 .83-.869.83.83 0 0 0-.744.468l-1.938-.41a.2.2 0 0 0-.153.028.2.2 0 0 0-.086.134l-.592 2.788c-1.24.038-2.358.41-3.17.992-.21-.2-.496-.324-.81-.324a1.163 1.163 0 0 0-.478 2.224q-.03.17-.029.353c0 1.795 2.091 3.256 4.669 3.256s4.668-1.451 4.668-3.256c0-.114-.01-.238-.029-.353.401-.181.688-.592.688-1.069 0-.65-.525-1.165-1.165-1.165"/></svg>
        </a>
      </nav>
    </div>
  </header>
  
          <section class="skill-page-header">
            <div class="skill-page-header-inner">
              <a href="/devops-cloud/" class="back-link">‚Üê Back to DevOps & Cloud</a>
              <div class="skill-page-meta">
                <a href="/devops-cloud/" class="skill-page-category">DevOps & Cloud</a>
                <span class="skill-page-author">by @thegovind</span>
              </div>
              <h1 class="skill-page-title">azure-ai-voicelive-py</h1>
              <p class="skill-page-desc">Build real-time voice AI applications</p>
            </div>
          </section>
          <div class="skill-page-body">
            <article class="skill-page-content">
              <div class="skill-source-wrapper">
                <div class="skill-source-header">
                  <div class="skill-source-title">Source Code</div>
                  <button class="copy-btn" onclick="copySkillContent(this)">
                    <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="9" y="9" width="13" height="13" rx="2" ry="2"/><path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"/></svg>
                    Copy
                  </button>
                </div>
                <div class="markdown-content">
                  <h1>Azure AI Voice Live SDK</h1>
<p>Build real-time voice AI applications with bidirectional WebSocket communication.</p>
<h2>Installation</h2>
<pre><code class="language-bash">pip install azure-ai-voicelive aiohttp azure-identity
</code></pre>
<h2>Environment Variables</h2>
<pre><code class="language-bash">AZURE_COGNITIVE_SERVICES_ENDPOINT=https://&lt;region&gt;.api.cognitive.microsoft.com
# For API key auth (not recommended for production)
AZURE_COGNITIVE_SERVICES_KEY=&lt;api-key&gt;
</code></pre>
<h2>Authentication</h2>
<p><strong>DefaultAzureCredential (preferred)</strong>:</p>
<pre><code class="language-python">from azure.ai.voicelive.aio import connect
from azure.identity.aio import DefaultAzureCredential

async with connect(
    endpoint=os.environ[&quot;AZURE_COGNITIVE_SERVICES_ENDPOINT&quot;],
    credential=DefaultAzureCredential(),
    model=&quot;gpt-4o-realtime-preview&quot;,
    credential_scopes=[&quot;https://cognitiveservices.azure.com/.default&quot;]
) as conn:
    ...
</code></pre>
<p><strong>API Key</strong>:</p>
<pre><code class="language-python">from azure.ai.voicelive.aio import connect
from azure.core.credentials import AzureKeyCredential

async with connect(
    endpoint=os.environ[&quot;AZURE_COGNITIVE_SERVICES_ENDPOINT&quot;],
    credential=AzureKeyCredential(os.environ[&quot;AZURE_COGNITIVE_SERVICES_KEY&quot;]),
    model=&quot;gpt-4o-realtime-preview&quot;
) as conn:
    ...
</code></pre>
<h2>Quick Start</h2>
<pre><code class="language-python">import asyncio
import os
from azure.ai.voicelive.aio import connect
from azure.identity.aio import DefaultAzureCredential

async def main():
    async with connect(
        endpoint=os.environ[&quot;AZURE_COGNITIVE_SERVICES_ENDPOINT&quot;],
        credential=DefaultAzureCredential(),
        model=&quot;gpt-4o-realtime-preview&quot;,
        credential_scopes=[&quot;https://cognitiveservices.azure.com/.default&quot;]
    ) as conn:
        # Update session with instructions
        await conn.session.update(session={
            &quot;instructions&quot;: &quot;You are a helpful assistant.&quot;,
            &quot;modalities&quot;: [&quot;text&quot;, &quot;audio&quot;],
            &quot;voice&quot;: &quot;alloy&quot;
        })
        
        # Listen for events
        async for event in conn:
            print(f&quot;Event: {event.type}&quot;)
            if event.type == &quot;response.audio_transcript.done&quot;:
                print(f&quot;Transcript: {event.transcript}&quot;)
            elif event.type == &quot;response.done&quot;:
                break

asyncio.run(main())
</code></pre>
<h2>Core Architecture</h2>
<h3>Connection Resources</h3>
<p>The <code>VoiceLiveConnection</code> exposes these resources:</p>
<table>
<thead>
<tr>
<th>Resource</th>
<th>Purpose</th>
<th>Key Methods</th>
</tr>
</thead>
<tbody><tr>
<td><code>conn.session</code></td>
<td>Session configuration</td>
<td><code>update(session=...)</code></td>
</tr>
<tr>
<td><code>conn.response</code></td>
<td>Model responses</td>
<td><code>create()</code>, <code>cancel()</code></td>
</tr>
<tr>
<td><code>conn.input_audio_buffer</code></td>
<td>Audio input</td>
<td><code>append()</code>, <code>commit()</code>, <code>clear()</code></td>
</tr>
<tr>
<td><code>conn.output_audio_buffer</code></td>
<td>Audio output</td>
<td><code>clear()</code></td>
</tr>
<tr>
<td><code>conn.conversation</code></td>
<td>Conversation state</td>
<td><code>item.create()</code>, <code>item.delete()</code>, <code>item.truncate()</code></td>
</tr>
<tr>
<td><code>conn.transcription_session</code></td>
<td>Transcription config</td>
<td><code>update(session=...)</code></td>
</tr>
</tbody></table>
<h2>Session Configuration</h2>
<pre><code class="language-python">from azure.ai.voicelive.models import RequestSession, FunctionTool

await conn.session.update(session=RequestSession(
    instructions=&quot;You are a helpful voice assistant.&quot;,
    modalities=[&quot;text&quot;, &quot;audio&quot;],
    voice=&quot;alloy&quot;,  # or &quot;echo&quot;, &quot;shimmer&quot;, &quot;sage&quot;, etc.
    input_audio_format=&quot;pcm16&quot;,
    output_audio_format=&quot;pcm16&quot;,
    turn_detection={
        &quot;type&quot;: &quot;server_vad&quot;,
        &quot;threshold&quot;: 0.5,
        &quot;prefix_padding_ms&quot;: 300,
        &quot;silence_duration_ms&quot;: 500
    },
    tools=[
        FunctionTool(
            type=&quot;function&quot;,
            name=&quot;get_weather&quot;,
            description=&quot;Get current weather&quot;,
            parameters={
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {
                    &quot;location&quot;: {&quot;type&quot;: &quot;string&quot;}
                },
                &quot;required&quot;: [&quot;location&quot;]
            }
        )
    ]
))
</code></pre>
<h2>Audio Streaming</h2>
<h3>Send Audio (Base64 PCM16)</h3>
<pre><code class="language-python">import base64

# Read audio chunk (16-bit PCM, 24kHz mono)
audio_chunk = await read_audio_from_microphone()
b64_audio = base64.b64encode(audio_chunk).decode()

await conn.input_audio_buffer.append(audio=b64_audio)
</code></pre>
<h3>Receive Audio</h3>
<pre><code class="language-python">async for event in conn:
    if event.type == &quot;response.audio.delta&quot;:
        audio_bytes = base64.b64decode(event.delta)
        await play_audio(audio_bytes)
    elif event.type == &quot;response.audio.done&quot;:
        print(&quot;Audio complete&quot;)
</code></pre>
<h2>Event Handling</h2>
<pre><code class="language-python">async for event in conn:
    match event.type:
        # Session events
        case &quot;session.created&quot;:
            print(f&quot;Session: {event.session}&quot;)
        case &quot;session.updated&quot;:
            print(&quot;Session updated&quot;)
        
        # Audio input events
        case &quot;input_audio_buffer.speech_started&quot;:
            print(f&quot;Speech started at {event.audio_start_ms}ms&quot;)
        case &quot;input_audio_buffer.speech_stopped&quot;:
            print(f&quot;Speech stopped at {event.audio_end_ms}ms&quot;)
        
        # Transcription events
        case &quot;conversation.item.input_audio_transcription.completed&quot;:
            print(f&quot;User said: {event.transcript}&quot;)
        case &quot;conversation.item.input_audio_transcription.delta&quot;:
            print(f&quot;Partial: {event.delta}&quot;)
        
        # Response events
        case &quot;response.created&quot;:
            print(f&quot;Response started: {event.response.id}&quot;)
        case &quot;response.audio_transcript.delta&quot;:
            print(event.delta, end=&quot;&quot;, flush=True)
        case &quot;response.audio.delta&quot;:
            audio = base64.b64decode(event.delta)
        case &quot;response.done&quot;:
            print(f&quot;Response complete: {event.response.status}&quot;)
        
        # Function calls
        case &quot;response.function_call_arguments.done&quot;:
            result = handle_function(event.name, event.arguments)
            await conn.conversation.item.create(item={
                &quot;type&quot;: &quot;function_call_output&quot;,
                &quot;call_id&quot;: event.call_id,
                &quot;output&quot;: json.dumps(result)
            })
            await conn.response.create()
        
        # Errors
        case &quot;error&quot;:
            print(f&quot;Error: {event.error.message}&quot;)
</code></pre>
<h2>Common Patterns</h2>
<h3>Manual Turn Mode (No VAD)</h3>
<pre><code class="language-python">await conn.session.update(session={&quot;turn_detection&quot;: None})

# Manually control turns
await conn.input_audio_buffer.append(audio=b64_audio)
await conn.input_audio_buffer.commit()  # End of user turn
await conn.response.create()  # Trigger response
</code></pre>
<h3>Interrupt Handling</h3>
<pre><code class="language-python">async for event in conn:
    if event.type == &quot;input_audio_buffer.speech_started&quot;:
        # User interrupted - cancel current response
        await conn.response.cancel()
        await conn.output_audio_buffer.clear()
</code></pre>
<h3>Conversation History</h3>
<pre><code class="language-python"># Add system message
await conn.conversation.item.create(item={
    &quot;type&quot;: &quot;message&quot;,
    &quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: [{&quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: &quot;Be concise.&quot;}]
})

# Add user message
await conn.conversation.item.create(item={
    &quot;type&quot;: &quot;message&quot;,
    &quot;role&quot;: &quot;user&quot;, 
    &quot;content&quot;: [{&quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: &quot;Hello!&quot;}]
})

await conn.response.create()
</code></pre>
<h2>Voice Options</h2>
<table>
<thead>
<tr>
<th>Voice</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td><code>alloy</code></td>
<td>Neutral, balanced</td>
</tr>
<tr>
<td><code>echo</code></td>
<td>Warm, conversational</td>
</tr>
<tr>
<td><code>shimmer</code></td>
<td>Clear, professional</td>
</tr>
<tr>
<td><code>sage</code></td>
<td>Calm, authoritative</td>
</tr>
<tr>
<td><code>coral</code></td>
<td>Friendly, upbeat</td>
</tr>
<tr>
<td><code>ash</code></td>
<td>Deep, measured</td>
</tr>
<tr>
<td><code>ballad</code></td>
<td>Expressive</td>
</tr>
<tr>
<td><code>verse</code></td>
<td>Storytelling</td>
</tr>
</tbody></table>
<p>Azure voices: Use <code>AzureStandardVoice</code>, <code>AzureCustomVoice</code>, or <code>AzurePersonalVoice</code> models.</p>
<h2>Audio Formats</h2>
<table>
<thead>
<tr>
<th>Format</th>
<th>Sample Rate</th>
<th>Use Case</th>
</tr>
</thead>
<tbody><tr>
<td><code>pcm16</code></td>
<td>24kHz</td>
<td>Default, high quality</td>
</tr>
<tr>
<td><code>pcm16-8000hz</code></td>
<td>8kHz</td>
<td>Telephony</td>
</tr>
<tr>
<td><code>pcm16-16000hz</code></td>
<td>16kHz</td>
<td>Voice assistants</td>
</tr>
<tr>
<td><code>g711_ulaw</code></td>
<td>8kHz</td>
<td>Telephony (US)</td>
</tr>
<tr>
<td><code>g711_alaw</code></td>
<td>8kHz</td>
<td>Telephony (EU)</td>
</tr>
</tbody></table>
<h2>Turn Detection Options</h2>
<pre><code class="language-python"># Server VAD (default)
{&quot;type&quot;: &quot;server_vad&quot;, &quot;threshold&quot;: 0.5, &quot;silence_duration_ms&quot;: 500}

# Azure Semantic VAD (smarter detection)
{&quot;type&quot;: &quot;azure_semantic_vad&quot;}
{&quot;type&quot;: &quot;azure_semantic_vad_en&quot;}  # English optimized
{&quot;type&quot;: &quot;azure_semantic_vad_multilingual&quot;}
</code></pre>
<h2>Error Handling</h2>
<pre><code class="language-python">from azure.ai.voicelive.aio import ConnectionError, ConnectionClosed

try:
    async with connect(...) as conn:
        async for event in conn:
            if event.type == &quot;error&quot;:
                print(f&quot;API Error: {event.error.code} - {event.error.message}&quot;)
except ConnectionClosed as e:
    print(f&quot;Connection closed: {e.code} - {e.reason}&quot;)
except ConnectionError as e:
    print(f&quot;Connection error: {e}&quot;)
</code></pre>
<h2>References</h2>
<ul>
<li><strong>Detailed API Reference</strong>: See <a href="https://github.com/openclaw/skills/blob/main/skills/thegovind/azure-ai-voicelive-py/references/api-reference.md">references/api-reference.md</a></li>
<li><strong>Complete Examples</strong>: See <a href="https://github.com/openclaw/skills/blob/main/skills/thegovind/azure-ai-voicelive-py/references/examples.md">references/examples.md</a></li>
<li><strong>All Models &amp; Types</strong>: See <a href="https://github.com/openclaw/skills/blob/main/skills/thegovind/azure-ai-voicelive-py/references/models.md">references/models.md</a></li>
</ul>

                </div>
              </div>
            </article>
            <aside class="skill-page-sidebar">
              <div class="sidebar-card">
                <div class="sidebar-title">Actions</div>
                <a href="https://github.com/openclaw/skills/tree/main/skills/thegovind/azure-ai-voicelive-py/SKILL.md" target="_blank" rel="noopener" class="sidebar-btn sidebar-btn-primary">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/></svg>
                  View on GitHub
                </a>
                <a href="https://clawdhub.com/thegovind/azure-ai-voicelive-py" target="_blank" rel="noopener" class="sidebar-btn sidebar-btn-clawdhub">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"/><line x1="2" y1="12" x2="22" y2="12"/></svg>
                  View on ClawdHub
                </a>
                <a href="https://raw.githubusercontent.com/openclaw/skills/main/skills/thegovind/azure-ai-voicelive-py/SKILL.md" download="SKILL.md" class="sidebar-btn sidebar-btn-secondary">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"/><polyline points="7 10 12 15 17 10"/><line x1="12" y1="15" x2="12" y2="3"/></svg>
                  Download SKILL.md
                </a>
                <button onclick="sendToSecurityAuditor()" class="sidebar-btn sidebar-btn-security">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 22s8-4 8-10V5l-8-3-8 3v7c0 6 8 10 8 10z"/></svg>
                  Security Check
                </button>
              </div>
              <div class="sidebar-card">
                <div class="sidebar-title">Details</div>
                <div class="sidebar-info">
                  <div class="sidebar-info-item">
                    <span class="sidebar-info-label">Author</span>
                    <span class="sidebar-info-value">@thegovind</span>
                  </div>
                  <div class="sidebar-info-item">
                    <span class="sidebar-info-label">Category</span>
                    <span class="sidebar-info-value">DevOps & Cloud</span>
                  </div>
                </div>
              </div>
            </aside>
          </div>
        
  <footer class="footer">
    <div class="footer-inner">
      <p class="footer-text" style="margin-bottom: 8px;"><a href="https://github.com/neonone123/moltdirectory" target="_blank" rel="noopener" style="color: var(--accent); text-decoration: none;">View on GitHub</a></p>
      <p class="footer-text" style="opacity: 0.6; font-size: 13px;">OpenClawDirectory.com is a community-run project and is not affiliated with the official OpenClaw team or Peter Steinberger. We are just fans of the lobster.</p>
    </div>
  </footer>
  <script>
    function toggleTheme() {
      const html = document.documentElement;
      const currentTheme = html.getAttribute('data-theme');
      const newTheme = currentTheme === 'light' ? 'dark' : 'light';
      if (newTheme === 'light') {
        html.setAttribute('data-theme', 'light');
      } else {
        html.removeAttribute('data-theme');
      }
      localStorage.setItem('theme', newTheme);
    }
    
    function copySkillContent(btn) {
      const wrapper = btn.closest('.skill-source-wrapper');
      const content = wrapper.querySelector('.markdown-content');
      if (content) {
        const text = content.innerText || content.textContent;
        navigator.clipboard.writeText(text).then(() => {
          btn.classList.add('copied');
          btn.innerHTML = '<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="20 6 9 17 4 12"/></svg> Copied!';
          setTimeout(() => {
            btn.classList.remove('copied');
            btn.innerHTML = '<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="9" y="9" width="13" height="13" rx="2" ry="2"/><path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"/></svg> Copy';
          }, 2000);
        });
      }
    }

    async function sendToSecurityAuditor() {
      try {
        const contentElement = document.querySelector('.markdown-content');
        if (!contentElement) throw new Error('Could not find markdown content');
        const skillContent = contentElement.innerText || contentElement.textContent;
        localStorage.setItem('skillAuditContent', skillContent);
        window.open('/security-auditor', '_blank'); 
      } catch (error) {
        alert('Error: ' + error.message);
      }
    }
  </script>
</body>
</html>