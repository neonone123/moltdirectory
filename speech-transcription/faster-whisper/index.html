<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Local speech-to-text using faster-whisper. 4-6x faster than OpenAI Whisper with identical accuracy; GPU acceleration">
  <title>faster-whisper - OpenClaw Directory</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>ü¶û</text></svg>">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../../style.css">
  <script>
    // Apply saved theme before page renders to prevent flash
    (function() {
      const savedTheme = localStorage.getItem('theme') || 'dark';
      if (savedTheme === 'light') {
        document.documentElement.setAttribute('data-theme', 'light');
      }
    })();
  </script>
</head>
<body>
  <header class="header">
    <div class="header-inner">
      <a href="../../index.html" class="logo">

        <span class="logo-text">OpenClaw Directory</span>
      </a>
      <nav class="header-links">
        <a href="../../start-here/index.html" class="header-link">Start Here</a>
        <a href="/security-auditor" class="header-link">Security Auditor</a>
        <a href="https://github.com/neonone123/moltdirectory/issues/new?template=add-skill.yml" class="header-link" target="_blank" rel="noopener">Add Skill</a>
        <button class="theme-toggle" aria-label="Toggle theme" onclick="toggleTheme()">
          <svg class="icon-moon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/></svg>
          <svg class="icon-sun" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
        </button>
        <a href="https://www.reddit.com/r/OpenClawDirectory/" class="header-link" target="_blank" rel="noopener" aria-label="Community">
          <svg viewBox="0 0 16 16" fill="currentColor" width="28" height="28"><path d="M6.167 8a.83.83 0 0 0-.83.83c0 .459.372.84.83.831a.831.831 0 0 0 0-1.661m1.843 3.647c.315 0 1.403-.038 1.976-.611a.23.23 0 0 0 0-.306.213.213 0 0 0-.306 0c-.353.363-1.126.487-1.67.487-.545 0-1.308-.124-1.671-.487a.213.213 0 0 0-.306 0 .213.213 0 0 0 0 .306c.564.563 1.652.61 1.977.61zm.992-2.807c0 .458.373.83.831.83s.83-.381.83-.83a.831.831 0 0 0-1.66 0z"/><path d="M16 8A8 8 0 1 1 0 8a8 8 0 0 1 16 0m-3.828-1.165c-.315 0-.602.124-.812.325-.801-.573-1.9-.945-3.121-.993l.534-2.501 1.738.372a.83.83 0 1 0 .83-.869.83.83 0 0 0-.744.468l-1.938-.41a.2.2 0 0 0-.153.028.2.2 0 0 0-.086.134l-.592 2.788c-1.24.038-2.358.41-3.17.992-.21-.2-.496-.324-.81-.324a1.163 1.163 0 0 0-.478 2.224q-.03.17-.029.353c0 1.795 2.091 3.256 4.669 3.256s4.668-1.451 4.668-3.256c0-.114-.01-.238-.029-.353.401-.181.688-.592.688-1.069 0-.65-.525-1.165-1.165-1.165"/></svg>
        </a>
      </nav>
    </div>
  </header>
  
          <section class="skill-page-header">
            <div class="skill-page-header-inner">
              <a href="../index.html" class="back-link">‚Üê Back to Speech & Transcription</a>
              <div class="skill-page-meta">
                <a href="../index.html" class="skill-page-category">Speech & Transcription</a>
                <span class="skill-page-author">by @theplasmak</span>
              </div>
              <h1 class="skill-page-title">faster-whisper</h1>
              <p class="skill-page-desc">Local speech-to-text using faster-whisper. 4-6x faster than OpenAI Whisper with identical accuracy; GPU acceleration</p>
            </div>
          </section>
          <div class="skill-page-body">
            <article class="skill-page-content">
              <div class="skill-source-wrapper">
                <div class="skill-source-header">
                  <div class="skill-source-title">Source Code</div>
                  <button class="copy-btn" onclick="copySkillContent(this)">
                    <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="9" y="9" width="13" height="13" rx="2" ry="2"/><path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"/></svg>
                    Copy
                  </button>
                </div>
                <div class="markdown-content">
                  <h1>Faster Whisper</h1>
<p>Local speech-to-text using faster-whisper ‚Äî a CTranslate2 reimplementation of OpenAI&#39;s Whisper that runs <strong>4-6x faster</strong> with identical accuracy. With GPU acceleration, expect <strong>~20x realtime</strong> transcription (a 10-minute audio file in ~30 seconds).</p>
<h2>When to Use</h2>
<p>Use this skill when you need to:</p>
<ul>
<li><strong>Transcribe audio/video files</strong> ‚Äî meetings, interviews, podcasts, lectures, YouTube videos</li>
<li><strong>Convert speech to text locally</strong> ‚Äî no API costs, works offline (after model download)</li>
<li><strong>Batch process multiple audio files</strong> ‚Äî efficient for large collections</li>
<li><strong>Generate subtitles/captions</strong> ‚Äî word-level timestamps available</li>
<li><strong>Multilingual transcription</strong> ‚Äî supports 99+ languages with auto-detection</li>
</ul>
<p><strong>Trigger phrases:</strong> &quot;transcribe this audio&quot;, &quot;convert speech to text&quot;, &quot;what did they say&quot;, &quot;make a transcript&quot;, &quot;audio to text&quot;, &quot;subtitle this video&quot;</p>
<p><strong>When NOT to use:</strong></p>
<ul>
<li>Real-time/streaming transcription (use streaming-optimized tools instead)</li>
<li>Cloud-only environments without local compute</li>
<li>Files &lt;10 seconds where API call latency doesn&#39;t matter</li>
</ul>
<h2>Quick Reference</h2>
<table>
<thead>
<tr>
<th>Task</th>
<th>Command</th>
<th>Notes</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Basic transcription</strong></td>
<td><code>./scripts/transcribe audio.mp3</code></td>
<td>Uses default distil-large-v3</td>
</tr>
<tr>
<td><strong>Faster English</strong></td>
<td><code>./scripts/transcribe audio.mp3 --model distil-medium.en --language en</code></td>
<td>English-only, 6.8x faster</td>
</tr>
<tr>
<td><strong>Maximum accuracy</strong></td>
<td><code>./scripts/transcribe audio.mp3 --model large-v3-turbo --beam-size 10</code></td>
<td>Slower but best quality</td>
</tr>
<tr>
<td><strong>Word timestamps</strong></td>
<td><code>./scripts/transcribe audio.mp3 --word-timestamps</code></td>
<td>For subtitles/captions</td>
</tr>
<tr>
<td><strong>JSON output</strong></td>
<td><code>./scripts/transcribe audio.mp3 --json -o output.json</code></td>
<td>Programmatic access</td>
</tr>
<tr>
<td><strong>Multilingual</strong></td>
<td><code>./scripts/transcribe audio.mp3 --model large-v3-turbo</code></td>
<td>Auto-detects language</td>
</tr>
<tr>
<td><strong>Remove silence</strong></td>
<td><code>./scripts/transcribe audio.mp3 --vad</code></td>
<td>Voice activity detection</td>
</tr>
</tbody></table>
<h2>Model Selection</h2>
<p>Choose the right model for your needs:</p>
<pre><code class="language-dot">digraph model_selection {
    rankdir=LR;
    node [shape=box, style=rounded];

    start [label=&quot;Start&quot;, shape=doublecircle];
    need_accuracy [label=&quot;Need maximum\naccuracy?&quot;, shape=diamond];
    multilingual [label=&quot;Multilingual\ncontent?&quot;, shape=diamond];
    resource_constrained [label=&quot;Resource\nconstraints?&quot;, shape=diamond];

    large_v3 [label=&quot;large-v3\nor\nlarge-v3-turbo&quot;, style=&quot;rounded,filled&quot;, fillcolor=lightblue];
    large_turbo [label=&quot;large-v3-turbo&quot;, style=&quot;rounded,filled&quot;, fillcolor=lightblue];
    distil_large [label=&quot;distil-large-v3\n(default)&quot;, style=&quot;rounded,filled&quot;, fillcolor=lightgreen];
    distil_medium [label=&quot;distil-medium.en&quot;, style=&quot;rounded,filled&quot;, fillcolor=lightyellow];
    distil_small [label=&quot;distil-small.en&quot;, style=&quot;rounded,filled&quot;, fillcolor=lightyellow];

    start -&gt; need_accuracy;
    need_accuracy -&gt; large_v3 [label=&quot;yes&quot;];
    need_accuracy -&gt; multilingual [label=&quot;no&quot;];
    multilingual -&gt; large_turbo [label=&quot;yes&quot;];
    multilingual -&gt; resource_constrained [label=&quot;no (English)&quot;];
    resource_constrained -&gt; distil_small [label=&quot;mobile/edge&quot;];
    resource_constrained -&gt; distil_medium [label=&quot;some limits&quot;];
    resource_constrained -&gt; distil_large [label=&quot;no&quot;];
}
</code></pre>
<h3>Model Table</h3>
<h4>Standard Models (Full Whisper)</h4>
<table>
<thead>
<tr>
<th>Model</th>
<th>Size</th>
<th>Speed</th>
<th>Accuracy</th>
<th>Use Case</th>
</tr>
</thead>
<tbody><tr>
<td><code>tiny</code> / <code>tiny.en</code></td>
<td>39M</td>
<td>Fastest</td>
<td>Basic</td>
<td>Quick drafts</td>
</tr>
<tr>
<td><code>base</code> / <code>base.en</code></td>
<td>74M</td>
<td>Very fast</td>
<td>Good</td>
<td>General use</td>
</tr>
<tr>
<td><code>small</code> / <code>small.en</code></td>
<td>244M</td>
<td>Fast</td>
<td>Better</td>
<td>Most tasks</td>
</tr>
<tr>
<td><code>medium</code> / <code>medium.en</code></td>
<td>769M</td>
<td>Moderate</td>
<td>High</td>
<td>Quality transcription</td>
</tr>
<tr>
<td><code>large-v1/v2/v3</code></td>
<td>1.5GB</td>
<td>Slower</td>
<td>Best</td>
<td>Maximum accuracy</td>
</tr>
<tr>
<td><strong><code>large-v3-turbo</code></strong></td>
<td>809M</td>
<td>Fast</td>
<td>Excellent</td>
<td><strong>Recommended for accuracy</strong></td>
</tr>
</tbody></table>
<h4>Distilled Models (~6x Faster, ~1% WER difference)</h4>
<table>
<thead>
<tr>
<th>Model</th>
<th>Size</th>
<th>Speed vs Standard</th>
<th>Accuracy</th>
<th>Use Case</th>
</tr>
</thead>
<tbody><tr>
<td><strong><code>distil-large-v3</code></strong></td>
<td>756M</td>
<td>~6.3x faster</td>
<td>9.7% WER</td>
<td><strong>Default, best balance</strong></td>
</tr>
<tr>
<td><code>distil-large-v2</code></td>
<td>756M</td>
<td>~5.8x faster</td>
<td>10.1% WER</td>
<td>Fallback</td>
</tr>
<tr>
<td><code>distil-medium.en</code></td>
<td>394M</td>
<td>~6.8x faster</td>
<td>11.1% WER</td>
<td>English-only, resource-constrained</td>
</tr>
<tr>
<td><code>distil-small.en</code></td>
<td>166M</td>
<td>~5.6x faster</td>
<td>12.1% WER</td>
<td>Mobile/edge devices</td>
</tr>
</tbody></table>
<p><code>.en</code> models are English-only and slightly faster/better for English content.</p>
<h2>Setup</h2>
<h3>Linux / macOS / WSL2</h3>
<pre><code class="language-bash"># Run the setup script (creates venv, installs deps, auto-detects GPU)
./setup.sh
</code></pre>
<h3>Windows (Native)</h3>
<pre><code class="language-powershell"># Run from PowerShell (auto-installs Python &amp; ffmpeg if missing via winget)
.\setup.ps1
</code></pre>
<p>The Windows setup script will:</p>
<ul>
<li>Auto-install Python 3.12 via winget if not found</li>
<li>Auto-install ffmpeg via winget if not found</li>
<li>Detect NVIDIA GPU and install CUDA-enabled PyTorch</li>
<li>Create venv and install all dependencies</li>
</ul>
<p>Requirements:</p>
<ul>
<li><strong>Linux/macOS/WSL2</strong>: Python 3.10+, ffmpeg</li>
<li><strong>Windows</strong>: Nothing! Setup auto-installs prerequisites via winget</li>
</ul>
<h3>Platform Support</h3>
<table>
<thead>
<tr>
<th>Platform</th>
<th>Acceleration</th>
<th>Speed</th>
<th>Auto-Install</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Windows + NVIDIA GPU</strong></td>
<td>CUDA</td>
<td>~20x realtime üöÄ</td>
<td>‚úÖ Full</td>
</tr>
<tr>
<td><strong>Linux + NVIDIA GPU</strong></td>
<td>CUDA</td>
<td>~20x realtime üöÄ</td>
<td>Manual prereqs</td>
</tr>
<tr>
<td><strong>WSL2 + NVIDIA GPU</strong></td>
<td>CUDA</td>
<td>~20x realtime üöÄ</td>
<td>Manual prereqs</td>
</tr>
<tr>
<td>macOS Apple Silicon</td>
<td>CPU*</td>
<td>~3-5x realtime</td>
<td>Manual prereqs</td>
</tr>
<tr>
<td>macOS Intel</td>
<td>CPU</td>
<td>~1-2x realtime</td>
<td>Manual prereqs</td>
</tr>
<tr>
<td>Windows (no GPU)</td>
<td>CPU</td>
<td>~1x realtime</td>
<td>‚úÖ Full</td>
</tr>
<tr>
<td>Linux (no GPU)</td>
<td>CPU</td>
<td>~1x realtime</td>
<td>Manual prereqs</td>
</tr>
</tbody></table>
<p>*faster-whisper uses CTranslate2 which is CPU-only on macOS, but Apple Silicon is fast enough for practical use.</p>
<h3>GPU Support (IMPORTANT!)</h3>
<p>The setup script auto-detects your GPU and installs PyTorch with CUDA. <strong>Always use GPU if available</strong> ‚Äî CPU transcription is extremely slow.</p>
<table>
<thead>
<tr>
<th>Hardware</th>
<th>Speed</th>
<th>9-min video</th>
</tr>
</thead>
<tbody><tr>
<td>RTX 3070 (GPU)</td>
<td>~20x realtime</td>
<td>~27 sec</td>
</tr>
<tr>
<td>CPU (int8)</td>
<td>~0.3x realtime</td>
<td>~30 min</td>
</tr>
</tbody></table>
<p>If setup didn&#39;t detect your GPU, manually install PyTorch with CUDA:</p>
<p><strong>Linux/macOS/WSL2:</strong></p>
<pre><code class="language-bash"># For CUDA 12.x
uv pip install --python .venv/bin/python torch --index-url https://download.pytorch.org/whl/cu121

# For CUDA 11.x
uv pip install --python .venv/bin/python torch --index-url https://download.pytorch.org/whl/cu118
</code></pre>
<p><strong>Windows:</strong></p>
<pre><code class="language-powershell"># For CUDA 12.x
.venv\Scripts\pip install torch --index-url https://download.pytorch.org/whl/cu121

# For CUDA 11.x
.venv\Scripts\pip install torch --index-url https://download.pytorch.org/whl/cu118
</code></pre>
<ul>
<li><strong>Windows users</strong>: Ensure you have <a href="https://www.nvidia.com/download/index.aspx">NVIDIA drivers</a> installed</li>
<li><strong>WSL2 users</strong>: Ensure you have the <a href="https://docs.nvidia.com/cuda/wsl-user-guide/">NVIDIA CUDA drivers for WSL</a> installed on Windows</li>
</ul>
<h2>Usage</h2>
<p><strong>Linux/macOS/WSL2:</strong></p>
<pre><code class="language-bash"># Basic transcription
./scripts/transcribe audio.mp3

# With specific model
./scripts/transcribe audio.wav --model large-v3-turbo

# With word timestamps
./scripts/transcribe audio.mp3 --word-timestamps

# Specify language (faster than auto-detect)
./scripts/transcribe audio.mp3 --language en

# JSON output
./scripts/transcribe audio.mp3 --json
</code></pre>
<p><strong>Windows (cmd or PowerShell):</strong></p>
<pre><code class="language-powershell"># Basic transcription
.\scripts\transcribe.cmd audio.mp3

# With specific model
.\scripts\transcribe.cmd audio.wav --model large-v3-turbo

# With word timestamps (PowerShell native syntax also works)
.\scripts\transcribe.ps1 audio.mp3 -WordTimestamps

# JSON output
.\scripts\transcribe.cmd audio.mp3 --json
</code></pre>
<h2>Options</h2>
<pre><code>--model, -m        Model name (default: distil-large-v3)
--language, -l     Language code (e.g., en, es, fr - auto-detect if omitted)
--word-timestamps  Include word-level timestamps
--beam-size        Beam search size (default: 5, higher = more accurate but slower)
--vad              Enable voice activity detection (removes silence)
--json, -j         Output as JSON
--output, -o       Save transcript to file
--device           cpu or cuda (auto-detected)
--compute-type     int8, float16, float32 (default: auto-optimized)
--quiet, -q        Suppress progress messages
</code></pre>
<h2>Examples</h2>
<pre><code class="language-bash"># Transcribe YouTube audio (after extraction with yt-dlp)
yt-dlp -x --audio-format mp3 &lt;URL&gt; -o audio.mp3
./scripts/transcribe audio.mp3

# Batch transcription with JSON output
for file in *.mp3; do
  ./scripts/transcribe &quot;$file&quot; --json &gt; &quot;${file%.mp3}.json&quot;
done

# High-accuracy transcription with larger beam size
./scripts/transcribe audio.mp3 \
  --model large-v3-turbo --beam-size 10 --word-timestamps

# Fast English-only transcription
./scripts/transcribe audio.mp3 \
  --model distil-medium.en --language en

# Transcribe with VAD (removes silence)
./scripts/transcribe audio.mp3 --vad
</code></pre>
<h2>Common Mistakes</h2>
<table>
<thead>
<tr>
<th>Mistake</th>
<th>Problem</th>
<th>Solution</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Using CPU when GPU available</strong></td>
<td>10-20x slower transcription</td>
<td>Check <code>nvidia-smi</code> on Windows/Linux; verify CUDA installation</td>
</tr>
<tr>
<td><strong>Not specifying language</strong></td>
<td>Wastes time auto-detecting on known content</td>
<td>Use <code>--language en</code> when you know the language</td>
</tr>
<tr>
<td><strong>Using wrong model</strong></td>
<td>Unnecessary slowness or poor accuracy</td>
<td>Default <code>distil-large-v3</code> is excellent; only use <code>large-v3</code> if accuracy issues</td>
</tr>
<tr>
<td><strong>Ignoring distilled models</strong></td>
<td>Missing 6x speedup with &lt;1% accuracy loss</td>
<td>Try <code>distil-large-v3</code> before reaching for standard models</td>
</tr>
<tr>
<td><strong>Forgetting ffmpeg</strong></td>
<td>Setup fails or audio can&#39;t be processed</td>
<td>Setup script handles this; manual installs need ffmpeg separately</td>
</tr>
<tr>
<td><strong>Out of memory errors</strong></td>
<td>Model too large for available VRAM/RAM</td>
<td>Use smaller model or <code>--compute-type int8</code></td>
</tr>
<tr>
<td><strong>Over-engineering beam size</strong></td>
<td>Diminishing returns past beam-size 5-7</td>
<td>Default 5 is fine; try 10 for critical transcripts</td>
</tr>
</tbody></table>
<h2>Performance Notes</h2>
<ul>
<li><strong>First run</strong>: Downloads model to <code>~/.cache/huggingface/</code> (one-time)</li>
<li><strong>GPU</strong>: Automatically uses CUDA if available (~10-20x faster)</li>
<li><strong>Quantization</strong>: INT8 used on CPU for ~4x speedup with minimal accuracy loss</li>
<li><strong>Memory</strong>:<ul>
<li><code>distil-large-v3</code>: ~2GB RAM / ~1GB VRAM</li>
<li><code>large-v3-turbo</code>: ~4GB RAM / ~2GB VRAM</li>
<li><code>tiny/base</code>: &lt;1GB RAM</li>
</ul>
</li>
</ul>
<h2>Why faster-whisper?</h2>
<ul>
<li><strong>Speed</strong>: ~4-6x faster than OpenAI&#39;s original Whisper</li>
<li><strong>Accuracy</strong>: Identical (uses same model weights)</li>
<li><strong>Efficiency</strong>: Lower memory usage via quantization</li>
<li><strong>Production-ready</strong>: Stable C++ backend (CTranslate2)</li>
<li><strong>Distilled models</strong>: ~6x faster with &lt;1% accuracy loss</li>
</ul>
<h2>Troubleshooting</h2>
<p><strong>&quot;CUDA not available ‚Äî using CPU&quot;</strong>: Install PyTorch with CUDA (see GPU Support above)
<strong>Setup fails</strong>: Make sure Python 3.10+ is installed
<strong>Out of memory</strong>: Use smaller model or <code>--compute-type int8</code>
<strong>Slow on CPU</strong>: Expected ‚Äî use GPU for practical transcription
<strong>Model download fails</strong>: Check <code>~/.cache/huggingface/</code> permissions (Linux/macOS) or <code>%USERPROFILE%\.cache\huggingface\</code> (Windows)</p>
<h3>Windows-Specific</h3>
<p><strong>&quot;winget not found&quot;</strong>: Install <a href="https://apps.microsoft.com/detail/9NBLGGH4NNS1">App Installer</a> from Microsoft Store, or install Python/ffmpeg manually
<strong>&quot;Python not in PATH after install&quot;</strong>: Close and reopen your terminal, then run <code>setup.ps1</code> again
<strong>PowerShell execution policy error</strong>: Run <code>Set-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy RemoteSigned</code> or use <code>transcribe.cmd</code> instead
<strong>nvidia-smi not found but have GPU</strong>: Install <a href="https://www.nvidia.com/download/index.aspx">NVIDIA drivers</a> ‚Äî the Game Ready or Studio drivers include nvidia-smi</p>
<h2>References</h2>
<ul>
<li><a href="https://github.com/SYSTRAN/faster-whisper">faster-whisper GitHub</a></li>
<li><a href="https://arxiv.org/abs/2311.00430">Distil-Whisper Paper</a></li>
<li><a href="https://huggingface.co/collections/Systran/faster-whisper">HuggingFace Models</a></li>
</ul>

                </div>
              </div>
            </article>
            <aside class="skill-page-sidebar">
              <div class="sidebar-card">
                <div class="sidebar-title">Actions</div>
                <a href="https://github.com/openclaw/skills/tree/main/skills/theplasmak/faster-whisper/SKILL.md" target="_blank" rel="noopener" class="sidebar-btn sidebar-btn-primary">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/></svg>
                  View on GitHub
                </a>
                <a href="https://clawdhub.com/theplasmak/faster-whisper" target="_blank" rel="noopener" class="sidebar-btn sidebar-btn-clawdhub">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"/><line x1="2" y1="12" x2="22" y2="12"/></svg>
                  View on ClawdHub
                </a>
                <a href="https://raw.githubusercontent.com/openclaw/skills/main/skills/theplasmak/faster-whisper/SKILL.md" download="SKILL.md" class="sidebar-btn sidebar-btn-secondary">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"/><polyline points="7 10 12 15 17 10"/><line x1="12" y1="15" x2="12" y2="3"/></svg>
                  Download SKILL.md
                </a>
                <button onclick="sendToSecurityAuditor()" class="sidebar-btn sidebar-btn-security">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 22s8-4 8-10V5l-8-3-8 3v7c0 6 8 10 8 10z"/></svg>
                  Security Check
                </button>
              </div>
              <div class="sidebar-card">
                <div class="sidebar-title">Details</div>
                <div class="sidebar-info">
                  <div class="sidebar-info-item">
                    <span class="sidebar-info-label">Author</span>
                    <span class="sidebar-info-value">@theplasmak</span>
                  </div>
                  <div class="sidebar-info-item">
                    <span class="sidebar-info-label">Category</span>
                    <span class="sidebar-info-value">Speech & Transcription</span>
                  </div>
                </div>
              </div>
            </aside>
          </div>
        
  <footer class="footer">
    <div class="footer-inner">
      <p class="footer-text" style="margin-bottom: 8px;"><a href="https://github.com/neonone123/moltdirectory" target="_blank" rel="noopener" style="color: var(--accent); text-decoration: none;">View on GitHub</a></p>
      <p class="footer-text" style="opacity: 0.6; font-size: 13px;">OpenClawDirectory.com is a community-run project and is not affiliated with the official OpenClaw team or Peter Steinberger. We are just fans of the lobster.</p>
    </div>
  </footer>
  <script>
    function toggleTheme() {
      const html = document.documentElement;
      const currentTheme = html.getAttribute('data-theme');
      const newTheme = currentTheme === 'light' ? 'dark' : 'light';
      if (newTheme === 'light') {
        html.setAttribute('data-theme', 'light');
      } else {
        html.removeAttribute('data-theme');
      }
      localStorage.setItem('theme', newTheme);
    }
    
    function copySkillContent(btn) {
      const wrapper = btn.closest('.skill-source-wrapper');
      const content = wrapper.querySelector('.markdown-content');
      if (content) {
        const text = content.innerText || content.textContent;
        navigator.clipboard.writeText(text).then(() => {
          btn.classList.add('copied');
          btn.innerHTML = '<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="20 6 9 17 4 12"/></svg> Copied!';
          setTimeout(() => {
            btn.classList.remove('copied');
            btn.innerHTML = '<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="9" y="9" width="13" height="13" rx="2" ry="2"/><path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"/></svg> Copy';
          }, 2000);
        });
      }
    }

    async function sendToSecurityAuditor() {
      try {
        const contentElement = document.querySelector('.markdown-content');
        if (!contentElement) throw new Error('Could not find markdown content');
        const skillContent = contentElement.innerText || contentElement.textContent;
        localStorage.setItem('skillAuditContent', skillContent);
        window.open('/security-auditor', '_blank'); 
      } catch (error) {
        alert('Error: ' + error.message);
      }
    }
  </script>
</body>
</html>