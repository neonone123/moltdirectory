<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Computer vision engineering skill">
  <title>senior-computer-vision - OpenClaw Directory</title>
  <link rel="canonical" href="https://moltdirectory.com/smart-home-iot/senior-computer-vision/">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>ü¶û</text></svg>">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/style.css">
  <script>
    // Apply saved theme before page renders to prevent flash
    (function() {
      const savedTheme = localStorage.getItem('theme') || 'dark';
      if (savedTheme === 'light') {
        document.documentElement.setAttribute('data-theme', 'light');
      }
    })();
  </script>
</head>
<body>
  <header class="header">
    <div class="header-inner">
      <a href="/" class="logo">

        <span class="logo-text">OpenClaw Directory</span>
      </a>
      <nav class="header-links">
        <a href="/start-here/" class="header-link">Start Here</a>
        <a href="/security-auditor" class="header-link">Security Auditor</a>
        <a href="https://github.com/neonone123/moltdirectory/issues/new?template=add-skill.yml" class="header-link" target="_blank" rel="noopener">Add Skill</a>
        <button class="theme-toggle" aria-label="Toggle theme" onclick="toggleTheme()">
          <svg class="icon-moon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/></svg>
          <svg class="icon-sun" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
        </button>
        <a href="https://www.reddit.com/r/OpenClawDirectory/" class="header-link" target="_blank" rel="noopener" aria-label="Community">
          <svg viewBox="0 0 16 16" fill="currentColor" width="28" height="28"><path d="M6.167 8a.83.83 0 0 0-.83.83c0 .459.372.84.83.831a.831.831 0 0 0 0-1.661m1.843 3.647c.315 0 1.403-.038 1.976-.611a.23.23 0 0 0 0-.306.213.213 0 0 0-.306 0c-.353.363-1.126.487-1.67.487-.545 0-1.308-.124-1.671-.487a.213.213 0 0 0-.306 0 .213.213 0 0 0 0 .306c.564.563 1.652.61 1.977.61zm.992-2.807c0 .458.373.83.831.83s.83-.381.83-.83a.831.831 0 0 0-1.66 0z"/><path d="M16 8A8 8 0 1 1 0 8a8 8 0 0 1 16 0m-3.828-1.165c-.315 0-.602.124-.812.325-.801-.573-1.9-.945-3.121-.993l.534-2.501 1.738.372a.83.83 0 1 0 .83-.869.83.83 0 0 0-.744.468l-1.938-.41a.2.2 0 0 0-.153.028.2.2 0 0 0-.086.134l-.592 2.788c-1.24.038-2.358.41-3.17.992-.21-.2-.496-.324-.81-.324a1.163 1.163 0 0 0-.478 2.224q-.03.17-.029.353c0 1.795 2.091 3.256 4.669 3.256s4.668-1.451 4.668-3.256c0-.114-.01-.238-.029-.353.401-.181.688-.592.688-1.069 0-.65-.525-1.165-1.165-1.165"/></svg>
        </a>
      </nav>
    </div>
  </header>
  
          <section class="skill-page-header">
            <div class="skill-page-header-inner">
              <a href="/smart-home-iot/" class="back-link">‚Üê Back to Smart Home & IoT</a>
              <div class="skill-page-meta">
                <a href="/smart-home-iot/" class="skill-page-category">Smart Home & IoT</a>
                <span class="skill-page-author">by @alirezarezvani</span>
              </div>
              <h1 class="skill-page-title">senior-computer-vision</h1>
              <p class="skill-page-desc">Computer vision engineering skill</p>
            </div>
          </section>
          <div class="skill-page-body">
            <article class="skill-page-content">
              <div class="skill-source-wrapper">
                <div class="skill-source-header">
                  <div class="skill-source-title">Source Code</div>
                  <button class="copy-btn" onclick="copySkillContent(this)">
                    <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="9" y="9" width="13" height="13" rx="2" ry="2"/><path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"/></svg>
                    Copy
                  </button>
                </div>
                <div class="markdown-content">
                  <h1>Senior Computer Vision Engineer</h1>
<p>Production computer vision engineering skill for object detection, image segmentation, and visual AI system deployment.</p>
<h2>Table of Contents</h2>
<ul>
<li><a href="#quick-start">Quick Start</a></li>
<li><a href="#core-expertise">Core Expertise</a></li>
<li><a href="#tech-stack">Tech Stack</a></li>
<li><a href="#workflow-1-object-detection-pipeline">Workflow 1: Object Detection Pipeline</a></li>
<li><a href="#workflow-2-model-optimization-and-deployment">Workflow 2: Model Optimization and Deployment</a></li>
<li><a href="#workflow-3-custom-dataset-preparation">Workflow 3: Custom Dataset Preparation</a></li>
<li><a href="#architecture-selection-guide">Architecture Selection Guide</a></li>
<li><a href="#reference-documentation">Reference Documentation</a></li>
<li><a href="#common-commands">Common Commands</a></li>
</ul>
<h2>Quick Start</h2>
<pre><code class="language-bash"># Generate training configuration for YOLO or Faster R-CNN
python scripts/vision_model_trainer.py models/ --task detection --arch yolov8

# Analyze model for optimization opportunities (quantization, pruning)
python scripts/inference_optimizer.py model.pt --target onnx --benchmark

# Build dataset pipeline with augmentations
python scripts/dataset_pipeline_builder.py images/ --format coco --augment
</code></pre>
<h2>Core Expertise</h2>
<p>This skill provides guidance on:</p>
<ul>
<li><strong>Object Detection</strong>: YOLO family (v5-v11), Faster R-CNN, DETR, RT-DETR</li>
<li><strong>Instance Segmentation</strong>: Mask R-CNN, YOLACT, SOLOv2</li>
<li><strong>Semantic Segmentation</strong>: DeepLabV3+, SegFormer, SAM (Segment Anything)</li>
<li><strong>Image Classification</strong>: ResNet, EfficientNet, Vision Transformers (ViT, DeiT)</li>
<li><strong>Video Analysis</strong>: Object tracking (ByteTrack, SORT), action recognition</li>
<li><strong>3D Vision</strong>: Depth estimation, point cloud processing, NeRF</li>
<li><strong>Production Deployment</strong>: ONNX, TensorRT, OpenVINO, CoreML</li>
</ul>
<h2>Tech Stack</h2>
<table>
<thead>
<tr>
<th>Category</th>
<th>Technologies</th>
</tr>
</thead>
<tbody><tr>
<td>Frameworks</td>
<td>PyTorch, torchvision, timm</td>
</tr>
<tr>
<td>Detection</td>
<td>Ultralytics (YOLO), Detectron2, MMDetection</td>
</tr>
<tr>
<td>Segmentation</td>
<td>segment-anything, mmsegmentation</td>
</tr>
<tr>
<td>Optimization</td>
<td>ONNX, TensorRT, OpenVINO, torch.compile</td>
</tr>
<tr>
<td>Image Processing</td>
<td>OpenCV, Pillow, albumentations</td>
</tr>
<tr>
<td>Annotation</td>
<td>CVAT, Label Studio, Roboflow</td>
</tr>
<tr>
<td>Experiment Tracking</td>
<td>MLflow, Weights &amp; Biases</td>
</tr>
<tr>
<td>Serving</td>
<td>Triton Inference Server, TorchServe</td>
</tr>
</tbody></table>
<h2>Workflow 1: Object Detection Pipeline</h2>
<p>Use this workflow when building an object detection system from scratch.</p>
<h3>Step 1: Define Detection Requirements</h3>
<p>Analyze the detection task requirements:</p>
<pre><code>Detection Requirements Analysis:
- Target objects: [list specific classes to detect]
- Real-time requirement: [yes/no, target FPS]
- Accuracy priority: [speed vs accuracy trade-off]
- Deployment target: [cloud GPU, edge device, mobile]
- Dataset size: [number of images, annotations per class]
</code></pre>
<h3>Step 2: Select Detection Architecture</h3>
<p>Choose architecture based on requirements:</p>
<table>
<thead>
<tr>
<th>Requirement</th>
<th>Recommended Architecture</th>
<th>Why</th>
</tr>
</thead>
<tbody><tr>
<td>Real-time (&gt;30 FPS)</td>
<td>YOLOv8/v11, RT-DETR</td>
<td>Single-stage, optimized for speed</td>
</tr>
<tr>
<td>High accuracy</td>
<td>Faster R-CNN, DINO</td>
<td>Two-stage, better localization</td>
</tr>
<tr>
<td>Small objects</td>
<td>YOLO + SAHI, Faster R-CNN + FPN</td>
<td>Multi-scale detection</td>
</tr>
<tr>
<td>Edge deployment</td>
<td>YOLOv8n, MobileNetV3-SSD</td>
<td>Lightweight architectures</td>
</tr>
<tr>
<td>Transformer-based</td>
<td>DETR, DINO, RT-DETR</td>
<td>End-to-end, no NMS required</td>
</tr>
</tbody></table>
<h3>Step 3: Prepare Dataset</h3>
<p>Convert annotations to required format:</p>
<pre><code class="language-bash"># COCO format (recommended)
python scripts/dataset_pipeline_builder.py data/images/ \
    --annotations data/labels/ \
    --format coco \
    --split 0.8 0.1 0.1 \
    --output data/coco/

# Verify dataset
python -c &quot;from pycocotools.coco import COCO; coco = COCO(&#39;data/coco/train.json&#39;); print(f&#39;Images: {len(coco.imgs)}, Categories: {len(coco.cats)}&#39;)&quot;
</code></pre>
<h3>Step 4: Configure Training</h3>
<p>Generate training configuration:</p>
<pre><code class="language-bash"># For Ultralytics YOLO
python scripts/vision_model_trainer.py data/coco/ \
    --task detection \
    --arch yolov8m \
    --epochs 100 \
    --batch 16 \
    --imgsz 640 \
    --output configs/

# For Detectron2
python scripts/vision_model_trainer.py data/coco/ \
    --task detection \
    --arch faster_rcnn_R_50_FPN \
    --framework detectron2 \
    --output configs/
</code></pre>
<h3>Step 5: Train and Validate</h3>
<pre><code class="language-bash"># Ultralytics training
yolo detect train data=data.yaml model=yolov8m.pt epochs=100 imgsz=640

# Detectron2 training
python train_net.py --config-file configs/faster_rcnn.yaml --num-gpus 1

# Validate on test set
yolo detect val model=runs/detect/train/weights/best.pt data=data.yaml
</code></pre>
<h3>Step 6: Evaluate Results</h3>
<p>Key metrics to analyze:</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Target</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>mAP@50</td>
<td>&gt;0.7</td>
<td>Mean Average Precision at IoU 0.5</td>
</tr>
<tr>
<td>mAP@50:95</td>
<td>&gt;0.5</td>
<td>COCO primary metric</td>
</tr>
<tr>
<td>Precision</td>
<td>&gt;0.8</td>
<td>Low false positives</td>
</tr>
<tr>
<td>Recall</td>
<td>&gt;0.8</td>
<td>Low missed detections</td>
</tr>
<tr>
<td>Inference time</td>
<td>&lt;33ms</td>
<td>For 30 FPS real-time</td>
</tr>
</tbody></table>
<h2>Workflow 2: Model Optimization and Deployment</h2>
<p>Use this workflow when preparing a trained model for production deployment.</p>
<h3>Step 1: Benchmark Baseline Performance</h3>
<pre><code class="language-bash"># Measure current model performance
python scripts/inference_optimizer.py model.pt \
    --benchmark \
    --input-size 640 640 \
    --batch-sizes 1 4 8 16 \
    --warmup 10 \
    --iterations 100
</code></pre>
<p>Expected output:</p>
<pre><code>Baseline Performance (PyTorch FP32):
- Batch 1: 45.2ms (22.1 FPS)
- Batch 4: 89.4ms (44.7 FPS)
- Batch 8: 165.3ms (48.4 FPS)
- Memory: 2.1 GB
- Parameters: 25.9M
</code></pre>
<h3>Step 2: Select Optimization Strategy</h3>
<table>
<thead>
<tr>
<th>Deployment Target</th>
<th>Optimization Path</th>
</tr>
</thead>
<tbody><tr>
<td>NVIDIA GPU (cloud)</td>
<td>PyTorch ‚Üí ONNX ‚Üí TensorRT FP16</td>
</tr>
<tr>
<td>NVIDIA GPU (edge)</td>
<td>PyTorch ‚Üí TensorRT INT8</td>
</tr>
<tr>
<td>Intel CPU</td>
<td>PyTorch ‚Üí ONNX ‚Üí OpenVINO</td>
</tr>
<tr>
<td>Apple Silicon</td>
<td>PyTorch ‚Üí CoreML</td>
</tr>
<tr>
<td>Generic CPU</td>
<td>PyTorch ‚Üí ONNX Runtime</td>
</tr>
<tr>
<td>Mobile</td>
<td>PyTorch ‚Üí TFLite or ONNX Mobile</td>
</tr>
</tbody></table>
<h3>Step 3: Export to ONNX</h3>
<pre><code class="language-bash"># Export with dynamic batch size
python scripts/inference_optimizer.py model.pt \
    --export onnx \
    --input-size 640 640 \
    --dynamic-batch \
    --simplify \
    --output model.onnx

# Verify ONNX model
python -c &quot;import onnx; model = onnx.load(&#39;model.onnx&#39;); onnx.checker.check_model(model); print(&#39;ONNX model valid&#39;)&quot;
</code></pre>
<h3>Step 4: Apply Quantization (Optional)</h3>
<p>For INT8 quantization with calibration:</p>
<pre><code class="language-bash"># Generate calibration dataset
python scripts/inference_optimizer.py model.onnx \
    --quantize int8 \
    --calibration-data data/calibration/ \
    --calibration-samples 500 \
    --output model_int8.onnx
</code></pre>
<p>Quantization impact analysis:</p>
<table>
<thead>
<tr>
<th>Precision</th>
<th>Size</th>
<th>Speed</th>
<th>Accuracy Drop</th>
</tr>
</thead>
<tbody><tr>
<td>FP32</td>
<td>100%</td>
<td>1x</td>
<td>0%</td>
</tr>
<tr>
<td>FP16</td>
<td>50%</td>
<td>1.5-2x</td>
<td>&lt;0.5%</td>
</tr>
<tr>
<td>INT8</td>
<td>25%</td>
<td>2-4x</td>
<td>1-3%</td>
</tr>
</tbody></table>
<h3>Step 5: Convert to Target Runtime</h3>
<pre><code class="language-bash"># TensorRT (NVIDIA GPU)
trtexec --onnx=model.onnx --saveEngine=model.engine --fp16

# OpenVINO (Intel)
mo --input_model model.onnx --output_dir openvino/

# CoreML (Apple)
python -c &quot;import coremltools as ct; model = ct.convert(&#39;model.onnx&#39;); model.save(&#39;model.mlpackage&#39;)&quot;
</code></pre>
<h3>Step 6: Benchmark Optimized Model</h3>
<pre><code class="language-bash">python scripts/inference_optimizer.py model.engine \
    --benchmark \
    --runtime tensorrt \
    --compare model.pt
</code></pre>
<p>Expected speedup:</p>
<pre><code>Optimization Results:
- Original (PyTorch FP32): 45.2ms
- Optimized (TensorRT FP16): 12.8ms
- Speedup: 3.5x
- Accuracy change: -0.3% mAP
</code></pre>
<h2>Workflow 3: Custom Dataset Preparation</h2>
<p>Use this workflow when preparing a computer vision dataset for training.</p>
<h3>Step 1: Audit Raw Data</h3>
<pre><code class="language-bash"># Analyze image dataset
python scripts/dataset_pipeline_builder.py data/raw/ \
    --analyze \
    --output analysis/
</code></pre>
<p>Analysis report includes:</p>
<pre><code>Dataset Analysis:
- Total images: 5,234
- Image sizes: 640x480 to 4096x3072 (variable)
- Formats: JPEG (4,891), PNG (343)
- Corrupted: 12 files
- Duplicates: 45 pairs

Annotation Analysis:
- Format detected: Pascal VOC XML
- Total annotations: 28,456
- Classes: 5 (car, person, bicycle, dog, cat)
- Distribution: car (12,340), person (8,234), bicycle (3,456), dog (2,890), cat (1,536)
- Empty images: 234
</code></pre>
<h3>Step 2: Clean and Validate</h3>
<pre><code class="language-bash"># Remove corrupted and duplicate images
python scripts/dataset_pipeline_builder.py data/raw/ \
    --clean \
    --remove-corrupted \
    --remove-duplicates \
    --output data/cleaned/
</code></pre>
<h3>Step 3: Convert Annotation Format</h3>
<pre><code class="language-bash"># Convert VOC to COCO format
python scripts/dataset_pipeline_builder.py data/cleaned/ \
    --annotations data/annotations/ \
    --input-format voc \
    --output-format coco \
    --output data/coco/
</code></pre>
<p>Supported format conversions:</p>
<table>
<thead>
<tr>
<th>From</th>
<th>To</th>
</tr>
</thead>
<tbody><tr>
<td>Pascal VOC XML</td>
<td>COCO JSON</td>
</tr>
<tr>
<td>YOLO TXT</td>
<td>COCO JSON</td>
</tr>
<tr>
<td>COCO JSON</td>
<td>YOLO TXT</td>
</tr>
<tr>
<td>LabelMe JSON</td>
<td>COCO JSON</td>
</tr>
<tr>
<td>CVAT XML</td>
<td>COCO JSON</td>
</tr>
</tbody></table>
<h3>Step 4: Apply Augmentations</h3>
<pre><code class="language-bash"># Generate augmentation config
python scripts/dataset_pipeline_builder.py data/coco/ \
    --augment \
    --aug-config configs/augmentation.yaml \
    --output data/augmented/
</code></pre>
<p>Recommended augmentations for detection:</p>
<pre><code class="language-yaml"># configs/augmentation.yaml
augmentations:
  geometric:
    - horizontal_flip: { p: 0.5 }
    - vertical_flip: { p: 0.1 }  # Only if orientation invariant
    - rotate: { limit: 15, p: 0.3 }
    - scale: { scale_limit: 0.2, p: 0.5 }

  color:
    - brightness_contrast: { brightness_limit: 0.2, contrast_limit: 0.2, p: 0.5 }
    - hue_saturation: { hue_shift_limit: 20, sat_shift_limit: 30, p: 0.3 }
    - blur: { blur_limit: 3, p: 0.1 }

  advanced:
    - mosaic: { p: 0.5 }  # YOLO-style mosaic
    - mixup: { p: 0.1 }   # Image mixing
    - cutout: { num_holes: 8, max_h_size: 32, max_w_size: 32, p: 0.3 }
</code></pre>
<h3>Step 5: Create Train/Val/Test Splits</h3>
<pre><code class="language-bash">python scripts/dataset_pipeline_builder.py data/augmented/ \
    --split 0.8 0.1 0.1 \
    --stratify \
    --seed 42 \
    --output data/final/
</code></pre>
<p>Split strategy guidelines:</p>
<table>
<thead>
<tr>
<th>Dataset Size</th>
<th>Train</th>
<th>Val</th>
<th>Test</th>
</tr>
</thead>
<tbody><tr>
<td>&lt;1,000 images</td>
<td>70%</td>
<td>15%</td>
<td>15%</td>
</tr>
<tr>
<td>1,000-10,000</td>
<td>80%</td>
<td>10%</td>
<td>10%</td>
</tr>
<tr>
<td>&gt;10,000</td>
<td>90%</td>
<td>5%</td>
<td>5%</td>
</tr>
</tbody></table>
<h3>Step 6: Generate Dataset Configuration</h3>
<pre><code class="language-bash"># For Ultralytics YOLO
python scripts/dataset_pipeline_builder.py data/final/ \
    --generate-config yolo \
    --output data.yaml

# For Detectron2
python scripts/dataset_pipeline_builder.py data/final/ \
    --generate-config detectron2 \
    --output detectron2_config.py
</code></pre>
<h2>Architecture Selection Guide</h2>
<h3>Object Detection Architectures</h3>
<table>
<thead>
<tr>
<th>Architecture</th>
<th>Speed</th>
<th>Accuracy</th>
<th>Best For</th>
</tr>
</thead>
<tbody><tr>
<td>YOLOv8n</td>
<td>1.2ms</td>
<td>37.3 mAP</td>
<td>Edge, mobile, real-time</td>
</tr>
<tr>
<td>YOLOv8s</td>
<td>2.1ms</td>
<td>44.9 mAP</td>
<td>Balanced speed/accuracy</td>
</tr>
<tr>
<td>YOLOv8m</td>
<td>4.2ms</td>
<td>50.2 mAP</td>
<td>General purpose</td>
</tr>
<tr>
<td>YOLOv8l</td>
<td>6.8ms</td>
<td>52.9 mAP</td>
<td>High accuracy</td>
</tr>
<tr>
<td>YOLOv8x</td>
<td>10.1ms</td>
<td>53.9 mAP</td>
<td>Maximum accuracy</td>
</tr>
<tr>
<td>RT-DETR-L</td>
<td>5.3ms</td>
<td>53.0 mAP</td>
<td>Transformer, no NMS</td>
</tr>
<tr>
<td>Faster R-CNN R50</td>
<td>46ms</td>
<td>40.2 mAP</td>
<td>Two-stage, high quality</td>
</tr>
<tr>
<td>DINO-4scale</td>
<td>85ms</td>
<td>49.0 mAP</td>
<td>SOTA transformer</td>
</tr>
</tbody></table>
<h3>Segmentation Architectures</h3>
<table>
<thead>
<tr>
<th>Architecture</th>
<th>Type</th>
<th>Speed</th>
<th>Best For</th>
</tr>
</thead>
<tbody><tr>
<td>YOLOv8-seg</td>
<td>Instance</td>
<td>4.5ms</td>
<td>Real-time instance seg</td>
</tr>
<tr>
<td>Mask R-CNN</td>
<td>Instance</td>
<td>67ms</td>
<td>High-quality masks</td>
</tr>
<tr>
<td>SAM</td>
<td>Promptable</td>
<td>50ms</td>
<td>Zero-shot segmentation</td>
</tr>
<tr>
<td>DeepLabV3+</td>
<td>Semantic</td>
<td>25ms</td>
<td>Scene parsing</td>
</tr>
<tr>
<td>SegFormer</td>
<td>Semantic</td>
<td>15ms</td>
<td>Efficient semantic seg</td>
</tr>
</tbody></table>
<h3>CNN vs Vision Transformer Trade-offs</h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>CNN (YOLO, R-CNN)</th>
<th>ViT (DETR, DINO)</th>
</tr>
</thead>
<tbody><tr>
<td>Training data needed</td>
<td>1K-10K images</td>
<td>10K-100K+ images</td>
</tr>
<tr>
<td>Training time</td>
<td>Fast</td>
<td>Slow (needs more epochs)</td>
</tr>
<tr>
<td>Inference speed</td>
<td>Faster</td>
<td>Slower</td>
</tr>
<tr>
<td>Small objects</td>
<td>Good with FPN</td>
<td>Needs multi-scale</td>
</tr>
<tr>
<td>Global context</td>
<td>Limited</td>
<td>Excellent</td>
</tr>
<tr>
<td>Positional encoding</td>
<td>Implicit</td>
<td>Explicit</td>
</tr>
</tbody></table>
<h2>Reference Documentation</h2>
<h3>1. Computer Vision Architectures</h3>
<p>See <code>references/computer_vision_architectures.md</code> for:</p>
<ul>
<li>CNN backbone architectures (ResNet, EfficientNet, ConvNeXt)</li>
<li>Vision Transformer variants (ViT, DeiT, Swin)</li>
<li>Detection heads (anchor-based vs anchor-free)</li>
<li>Feature Pyramid Networks (FPN, BiFPN, PANet)</li>
<li>Neck architectures for multi-scale detection</li>
</ul>
<h3>2. Object Detection Optimization</h3>
<p>See <code>references/object_detection_optimization.md</code> for:</p>
<ul>
<li>Non-Maximum Suppression variants (NMS, Soft-NMS, DIoU-NMS)</li>
<li>Anchor optimization and anchor-free alternatives</li>
<li>Loss function design (focal loss, GIoU, CIoU, DIoU)</li>
<li>Training strategies (warmup, cosine annealing, EMA)</li>
<li>Data augmentation for detection (mosaic, mixup, copy-paste)</li>
</ul>
<h3>3. Production Vision Systems</h3>
<p>See <code>references/production_vision_systems.md</code> for:</p>
<ul>
<li>ONNX export and optimization</li>
<li>TensorRT deployment pipeline</li>
<li>Batch inference optimization</li>
<li>Edge device deployment (Jetson, Intel NCS)</li>
<li>Model serving with Triton</li>
<li>Video processing pipelines</li>
</ul>
<h2>Common Commands</h2>
<h3>Ultralytics YOLO</h3>
<pre><code class="language-bash"># Training
yolo detect train data=coco.yaml model=yolov8m.pt epochs=100 imgsz=640

# Validation
yolo detect val model=best.pt data=coco.yaml

# Inference
yolo detect predict model=best.pt source=images/ save=True

# Export
yolo export model=best.pt format=onnx simplify=True dynamic=True
</code></pre>
<h3>Detectron2</h3>
<pre><code class="language-bash"># Training
python train_net.py --config-file configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml \
    --num-gpus 1 OUTPUT_DIR ./output

# Evaluation
python train_net.py --config-file configs/faster_rcnn.yaml --eval-only \
    MODEL.WEIGHTS output/model_final.pth

# Inference
python demo.py --config-file configs/faster_rcnn.yaml \
    --input images/*.jpg --output results/ \
    --opts MODEL.WEIGHTS output/model_final.pth
</code></pre>
<h3>MMDetection</h3>
<pre><code class="language-bash"># Training
python tools/train.py configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py

# Testing
python tools/test.py configs/faster_rcnn.py checkpoints/latest.pth --eval bbox

# Inference
python demo/image_demo.py demo.jpg configs/faster_rcnn.py checkpoints/latest.pth
</code></pre>
<h3>Model Optimization</h3>
<pre><code class="language-bash"># ONNX export and simplify
python -c &quot;import torch; model = torch.load(&#39;model.pt&#39;); torch.onnx.export(model, torch.randn(1,3,640,640), &#39;model.onnx&#39;, opset_version=17)&quot;
python -m onnxsim model.onnx model_sim.onnx

# TensorRT conversion
trtexec --onnx=model.onnx --saveEngine=model.engine --fp16 --workspace=4096

# Benchmark
trtexec --loadEngine=model.engine --batch=1 --iterations=1000 --avgRuns=100
</code></pre>
<h2>Performance Targets</h2>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Real-time</th>
<th>High Accuracy</th>
<th>Edge</th>
</tr>
</thead>
<tbody><tr>
<td>FPS</td>
<td>&gt;30</td>
<td>&gt;10</td>
<td>&gt;15</td>
</tr>
<tr>
<td>mAP@50</td>
<td>&gt;0.6</td>
<td>&gt;0.8</td>
<td>&gt;0.5</td>
</tr>
<tr>
<td>Latency P99</td>
<td>&lt;50ms</td>
<td>&lt;150ms</td>
<td>&lt;100ms</td>
</tr>
<tr>
<td>GPU Memory</td>
<td>&lt;4GB</td>
<td>&lt;8GB</td>
<td>&lt;2GB</td>
</tr>
<tr>
<td>Model Size</td>
<td>&lt;50MB</td>
<td>&lt;200MB</td>
<td>&lt;20MB</td>
</tr>
</tbody></table>
<h2>Resources</h2>
<ul>
<li><strong>Architecture Guide</strong>: <code>references/computer_vision_architectures.md</code></li>
<li><strong>Optimization Guide</strong>: <code>references/object_detection_optimization.md</code></li>
<li><strong>Deployment Guide</strong>: <code>references/production_vision_systems.md</code></li>
<li><strong>Scripts</strong>: <code>scripts/</code> directory for automation tools</li>
</ul>

                </div>
              </div>
            </article>
            <aside class="skill-page-sidebar">
              <div class="sidebar-card">
                <div class="sidebar-title">Actions</div>
                <a href="https://github.com/openclaw/skills/tree/main/skills/alirezarezvani/senior-computer-vision/SKILL.md" target="_blank" rel="noopener" class="sidebar-btn sidebar-btn-primary">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/></svg>
                  View on GitHub
                </a>
                <a href="https://clawdhub.com/alirezarezvani/senior-computer-vision" target="_blank" rel="noopener" class="sidebar-btn sidebar-btn-clawdhub">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"/><line x1="2" y1="12" x2="22" y2="12"/></svg>
                  View on ClawdHub
                </a>
                <a href="https://raw.githubusercontent.com/openclaw/skills/main/skills/alirezarezvani/senior-computer-vision/SKILL.md" download="SKILL.md" class="sidebar-btn sidebar-btn-secondary">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"/><polyline points="7 10 12 15 17 10"/><line x1="12" y1="15" x2="12" y2="3"/></svg>
                  Download SKILL.md
                </a>
                <button onclick="sendToSecurityAuditor()" class="sidebar-btn sidebar-btn-security">
                  <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 22s8-4 8-10V5l-8-3-8 3v7c0 6 8 10 8 10z"/></svg>
                  Security Check
                </button>
              </div>
              <div class="sidebar-card">
                <div class="sidebar-title">Details</div>
                <div class="sidebar-info">
                  <div class="sidebar-info-item">
                    <span class="sidebar-info-label">Author</span>
                    <span class="sidebar-info-value">@alirezarezvani</span>
                  </div>
                  <div class="sidebar-info-item">
                    <span class="sidebar-info-label">Category</span>
                    <span class="sidebar-info-value">Smart Home & IoT</span>
                  </div>
                </div>
              </div>
            </aside>
          </div>
        
  <footer class="footer">
    <div class="footer-inner">
      <p class="footer-text" style="margin-bottom: 8px;"><a href="https://github.com/neonone123/moltdirectory" target="_blank" rel="noopener" style="color: var(--accent); text-decoration: none;">View on GitHub</a></p>
      <p class="footer-text" style="opacity: 0.6; font-size: 13px;">OpenClawDirectory.com is a community-run project and is not affiliated with the official OpenClaw team or Peter Steinberger. We are just fans of the lobster.</p>
    </div>
  </footer>
  <script>
    function toggleTheme() {
      const html = document.documentElement;
      const currentTheme = html.getAttribute('data-theme');
      const newTheme = currentTheme === 'light' ? 'dark' : 'light';
      if (newTheme === 'light') {
        html.setAttribute('data-theme', 'light');
      } else {
        html.removeAttribute('data-theme');
      }
      localStorage.setItem('theme', newTheme);
    }
    
    function copySkillContent(btn) {
      const wrapper = btn.closest('.skill-source-wrapper');
      const content = wrapper.querySelector('.markdown-content');
      if (content) {
        const text = content.innerText || content.textContent;
        navigator.clipboard.writeText(text).then(() => {
          btn.classList.add('copied');
          btn.innerHTML = '<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="20 6 9 17 4 12"/></svg> Copied!';
          setTimeout(() => {
            btn.classList.remove('copied');
            btn.innerHTML = '<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="9" y="9" width="13" height="13" rx="2" ry="2"/><path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"/></svg> Copy';
          }, 2000);
        });
      }
    }

    async function sendToSecurityAuditor() {
      try {
        const contentElement = document.querySelector('.markdown-content');
        if (!contentElement) throw new Error('Could not find markdown content');
        const skillContent = contentElement.innerText || contentElement.textContent;
        localStorage.setItem('skillAuditContent', skillContent);
        window.open('/security-auditor', '_blank'); 
      } catch (error) {
        alert('Error: ' + error.message);
      }
    }
  </script>
</body>
</html>